[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "",
    "text": "Use the randomForest() function in the randomForest package to fit three random forest regressions (bagging algorithms) with the ntree argument set to, respectively, 50, 100, and 500 (number of trees to grow). In each case, compute the root mean squared error (RMSE) for the training and test datasets.\n\nThe following code prepares the dataset and sets the training and testing data:\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\nsuppressMessages(library(dplyr))\nsuppressMessages(library(splines))\nbike_data &lt;- read.csv('/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 2/bike_rental_hourly.csv')\nbike_data$log_cnt &lt;- log(bike_data$cnt)\nbike_data$hour &lt;- bike_data$hr/23 # transform [0, 23] to [0, 1]. 0 is midnight, 1 is 11 PM\n\n# One hot for weathersit\none_hot_encode_weathersit &lt;- model.matrix(~ as.factor(weathersit) - 1,data = bike_data)\none_hot_encode_weathersit  &lt;- one_hot_encode_weathersit[, -1] # Remove reference category\ncolnames(one_hot_encode_weathersit) &lt;- c('cloudy', 'light rain', 'heavy rain')\nbike_data &lt;- cbind(bike_data, one_hot_encode_weathersit)\n\n# One hot for weekday\none_hot_encode_weekday &lt;- model.matrix(~ as.factor(weekday) - 1,data = bike_data)\none_hot_encode_weekday  &lt;- one_hot_encode_weekday[, -1] # Remove reference category\ncolnames(one_hot_encode_weekday) &lt;- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')\nbike_data &lt;- cbind(bike_data, one_hot_encode_weekday)\n\n# One hot for weekday\none_hot_encode_season &lt;- model.matrix(~ as.factor(season) - 1,data = bike_data)\none_hot_encode_season  &lt;- one_hot_encode_season[, -1] # Remove reference category\ncolnames(one_hot_encode_season) &lt;- c('Spring', 'Summer', 'Fall')\nbike_data &lt;- cbind(bike_data, one_hot_encode_season)\n\n# Create lags\nbike_data_new &lt;- mutate(bike_data, lag1 = lag(log_cnt, 1), lag2 = lag(log_cnt, 2), \n                        lag3 = lag(log_cnt, 3), lag4 = lag(log_cnt, 4), lag24 = lag(log_cnt, 24))\n\nbike_data_new &lt;- bike_data_new[-c(1:24),] # Lost 24 obs because of lagging\n\n# Create training and test data\nbike_all_data_train &lt;- bike_data_new[bike_data_new$dteday &gt;= as.Date(\"2011-01-01\") & bike_data_new$dteday &lt;=  as.Date(\"2012-05-31\"), ]\nbike_all_data_test &lt;- bike_data_new[bike_data_new$dteday &gt;= as.Date(\"2012-06-01\") & bike_data_new$dteday &lt;=  as.Date(\"2012-12-31\"), ]\nX_train &lt;- cbind(1, bike_all_data_train[, c(\"lag1\", \"lag2\",  \"lag3\", \"lag4\", \"lag24\")])\nspline_basis &lt;- ns(bike_all_data_train$hour, df = 10, intercept = FALSE)\nX_train &lt;- cbind(X_train, spline_basis)\ncolnames(X_train)[1] &lt;- \"intercept\"\nknots &lt;- attr(spline_basis, \"knots\")\nvariables_to_keep_in_X &lt;- c(\"yr\", \"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\") \nvariables_to_keep_in_X &lt;- c(variables_to_keep_in_X, colnames(one_hot_encode_weathersit), colnames(one_hot_encode_weekday), colnames(one_hot_encode_season))\nX_train &lt;- cbind(X_train, bike_all_data_train[, variables_to_keep_in_X])\n# Training data\nX_train &lt;- as.matrix(X_train)\ny_train &lt;- bike_all_data_train$log_cnt\n# Test data\ny_test &lt;- bike_all_data_test$log_cnt\nX_test &lt;- cbind(1, bike_all_data_test[, c(\"lag1\", \"lag2\",  \"lag3\", \"lag4\", \"lag24\")])\nspline_basis_test &lt;- ns(bike_all_data_test$hour, df=10, knots=knots, intercept = FALSE)\nX_test &lt;- cbind(X_test, spline_basis_test)\ncolnames(X_test)[1] &lt;- \"intercept\"\nX_test &lt;- cbind(X_test, bike_all_data_test[, variables_to_keep_in_X])\nX_test &lt;- as.matrix(X_test)\n\nThe following code uses the randomForest() function in the randomForest package to fit three random forest regressions (bagging algorithms) with the ntree argument set to, respectively, 50, 100, and 500 (number of trees to grow). Then it computes the RMSE for the training and test datasets:\n\n# Load library\nsuppressMessages(library(randomForest))\n\n# Random Forest with ntree = 50\nset.seed(123)\nForest_cincuenta &lt;- randomForest(X_train, y_train, ntree = 50)\ny_train_cincuenta &lt;- predict(Forest_cincuenta, X_train)\ny_test_cincuenta &lt;- predict(Forest_cincuenta, X_test)\n\n# RMSE for ntree = 50\nrmse_train_cincuenta &lt;- sqrt(mean((y_train - y_train_cincuenta)^2))\nrmse_test_cincuenta &lt;- sqrt(mean((y_test - y_test_cincuenta)^2))\n\n# Random Forest with ntree = 100\nForest_cien &lt;- randomForest(X_train, y_train, ntree = 100)\ny_train_cien &lt;- predict(Forest_cien, X_train)\ny_test_cien &lt;- predict(Forest_cien, X_test)\n\n# RMSE for ntree = 100\nrmse_train_cien &lt;- sqrt(mean((y_train - y_train_cien)^2))\nrmse_test_cien &lt;- sqrt(mean((y_test - y_test_cien)^2))\n\n# Random Forest with ntree = 500\nForest_quinientos &lt;- randomForest(X_train, y_train, ntree = 500)\ny_train_quinientos &lt;- predict(Forest_quinientos, X_train)\ny_test_quinientos &lt;- predict(Forest_quinientos, X_test)\n\n# RMSE for ntree = 500\nrmse_train_quinientos &lt;- sqrt(mean((y_train - y_train_quinientos)^2))\nrmse_test_quinientos &lt;- sqrt(mean((y_test - y_test_quinientos)^2))\n\n# Print RMSE values\nprint(paste(\"RMSE for ntree = 50 on the training data:\", rmse_train_cincuenta))\n\n[1] \"RMSE for ntree = 50 on the training data: 0.143799442480079\"\n\nprint(paste(\"RMSE for ntree = 50 on the test data:\", rmse_test_cincuenta))\n\n[1] \"RMSE for ntree = 50 on the test data: 0.295419159165542\"\n\nprint(paste(\"RMSE for ntree = 100 on the training data:\", rmse_train_cien))\n\n[1] \"RMSE for ntree = 100 on the training data: 0.141731736751503\"\n\nprint(paste(\"RMSE for ntree = 100 on the test data:\", rmse_test_cien))\n\n[1] \"RMSE for ntree = 100 on the test data: 0.292150287649632\"\n\nprint(paste(\"RMSE for ntree = 500 on the training data:\", rmse_train_quinientos))\n\n[1] \"RMSE for ntree = 500 on the training data: 0.139092722085123\"\n\nprint(paste(\"RMSE for ntree = 500 on the test data:\", rmse_test_quinientos))\n\n[1] \"RMSE for ntree = 500 on the test data: 0.290405402095089\"\n\n\n\n\n\n\nPlot a time series plot of the response in the original scale (i.e.Â counts and not log-counts) for the last week of the test data (last \\(24\\times 7\\) observations). In the same figure, plot a time series plot of the fitted values (in the original scale) from Problem 1.1 when using ntree=50 and ntree=100. Comment on the results.\n\nThe next code plots a time series plot of the response in the original scale for the last week of the test data, in the same figure, plots a time series plot of the fitted values (in the original scale) from Problem 1.1 when using ntree=50 and ntree=100.:\n\n# Llast week test data\nlast_week_test &lt;- tail(bike_all_data_test, 24 * 7)\n\n# Predicted values for ntree = 50 and ntree = 100 in original scale\ny_test_pred_cincuenta_org &lt;- exp(y_test_cincuenta)\ny_test_pred_cien_org &lt;- exp(y_test_cien)\n\n# Last week of predicted values\nlast_week_pred_cincuenta &lt;- tail(y_test_pred_cincuenta_org, 24 * 7)\nlast_week_pred_cien &lt;- tail(y_test_pred_cien_org, 24 * 7)\n\n# Last week bike counts in the original scale\nlast_week_y_test &lt;- tail(exp(y_test), 24 * 7)\n\n# Plot\nplot(last_week_y_test, type = \"l\", col = \"green\", lwd = 2, xlab = \"Time in hours\", ylab = \"Number of bike rentals\", main = \"Time Series Plot of Actual and Predicted Bike Rentals for the Last Week\", cex.main = 0.8)\nlines(last_week_pred_cincuenta, col = \"cornflowerblue\", lwd = 2, lty = 2)\nlines(last_week_pred_cien, col = \"lightcoral\", lwd = 2, lty = 3)\n\n# Legend\nlegend(\"topleft\", legend = c(\"Actual counts\", \"Predicted (ntree = 50)\", \"Predicted (ntree = 100)\"), col = c(\"green\", \"cornflowerblue\", \"lightcoral\"), lty = c(1, 2, 3), lwd = 2, cex = 0.8)\n\n\n\n\n\n\n\n\nAs we can see in the chart, when we use a larger number of trees to grow, the predicted number of bike rides approximates better the actual number of bike rides in the peaks. This happens because more trees allow for better generalization and lower variance in the predictions, which results in the model capturing the peaks more accurately.\n\n\n\n\nUse the xgboost() function in the xgboost package to fit three extreme gradient boosting regressions (boosting algorithms) with the nrounds argument set to, respectively, 10, 25, and 50 (max number of boosting iterations). In each case, compute the root mean squared error (RMSE) for the training and test datasets.\n\nThe next code uses the xgboost() function in the xgboost package to fit three extreme gradient boosting regressions with the nrounds argument set to, respectively, 10, 25, and 50. Moreover, the code computes and prints the root mean squared error for the training and test datasets:\n\nsuppressMessages(library(xgboost))\n\n# Convert the data to a compact format for xgboost, we use y_train as label\nxgtrain &lt;- xgb.DMatrix(data = X_train, label = y_train)\n\n# Using xgboost with 10 iterations using a regression with squared loss\nxgb_diez &lt;- xgboost(data = xgtrain, nrounds = 10, objective = \"reg:squarederror\", verbose = 0)\n\n# Predict\ny_train_xgb_diez &lt;- predict(xgb_diez, newdata = X_train)\ny_test_xgb_diez &lt;- predict(xgb_diez, newdata = X_test)\n\n# Using xgboost with 25 iterations using a regression with squared loss\nxgb_venticinco &lt;- xgboost(data = xgtrain, nrounds = 25, objective = \"reg:squarederror\", verbose = 0)\n\n# Predict\ny_train_xgb_venticinco &lt;- predict(xgb_venticinco, newdata = X_train)\ny_test_xgb_venticinco &lt;- predict(xgb_venticinco, newdata = X_test)\n\n# Using xgboost with 50 iterations using a regression with squared loss\nxgb_cincuenta &lt;- xgboost(data = xgtrain, nrounds = 50, objective = \"reg:squarederror\", verbose = 0)\n\n# Predictions for training and test data\ny_train_xgb_cincuenta &lt;- predict(xgb_cincuenta, newdata = X_train)\ny_test_xgb_cincuenta &lt;- predict(xgb_cincuenta, newdata = X_test)\n\n# Calculating and printing RMSE values\nrmse_train_diez_xgb &lt;- sqrt(mean((y_train - y_train_xgb_diez)^2))\nrmse_test_diez_xgb &lt;- sqrt(mean((y_test - y_test_xgb_diez)^2))\nrmse_train_venticinco_xgb &lt;- sqrt(mean((y_train - y_train_xgb_venticinco)^2))\nrmse_test_venticinco_xgb &lt;- sqrt(mean((y_test - y_test_xgb_venticinco)^2))\nrmse_train_cincuenta_xgb &lt;- sqrt(mean((y_train - y_train_xgb_cincuenta)^2))\nrmse_test_cincuenta_xgb &lt;- sqrt(mean((y_test - y_test_xgb_cincuenta)^2))\nprint(paste(\"RMSE for nrounds = 10 on the training data:\", rmse_train_diez_xgb))\n\n[1] \"RMSE for nrounds = 10 on the training data: 0.323424139233079\"\n\nprint(paste(\"RMSE for nrounds = 10 on the test data:\", rmse_test_diez_xgb))\n\n[1] \"RMSE for nrounds = 10 on the test data: 0.367012312415504\"\n\nprint(paste(\"RMSE for nrounds = 25 on the training data:\", rmse_train_venticinco_xgb))\n\n[1] \"RMSE for nrounds = 25 on the training data: 0.254995729934359\"\n\nprint(paste(\"RMSE for nrounds = 25 on the test data:\", rmse_test_venticinco_xgb))\n\n[1] \"RMSE for nrounds = 25 on the test data: 0.302291709129745\"\n\nprint(paste(\"RMSE for nrounds = 50 on the training data:\", rmse_train_cincuenta_xgb))\n\n[1] \"RMSE for nrounds = 50 on the training data: 0.215796470201241\"\n\nprint(paste(\"RMSE for nrounds = 50 on the test data:\", rmse_test_cincuenta_xgb))\n\n[1] \"RMSE for nrounds = 50 on the test data: 0.300357225012064\"\n\n\n\n\n\n\nPlot the response in the original scale for the last week of the test data (last \\(24\\times 7\\) observations) vs the corresponding fitted values from the best bagging model in Problem 1.2 and the best boosting model in Problem 1.3. Comment on the results.\n\nThe next code plots the response in the original scale for the last week of the test data vs the corresponding fitted values from the best bagging model in Problem 1.2 (ntree =500, lowest RMSE on test data) and the best boosting model in Problem 1.3 (nrounds =50, lowest RMSE on test data):\n\n# Predicted values in original scale\nrf_top_original &lt;- exp(y_test_quinientos)\nxgb_top_original &lt;- exp(y_test_xgb_cincuenta)\n\n# Last week of predicted values for each model\nlast_week_rf_top_original &lt;- tail(rf_top_original, 24 * 7)\nlast_week_xgb_top_original &lt;- tail(xgb_top_original, 24 * 7)\n\n# Plot\nplot(last_week_y_test, type = \"l\", col = \"green\", lwd = 2, xlab = \"Time in hours\", ylab = \"Number of Bike Rentals\", \n     main = \"Actual vs Predicted Bike Rentals for the Last Week\", cex.main = 0.8, ylim = range(c(last_week_y_test, last_week_rf_top_original, last_week_xgb_top_original)))\nlines(last_week_rf_top_original, col = \"cornflowerblue\", lwd = 2, lty = 2)\nlines(last_week_xgb_top_original, col = \"lightcoral\", lwd = 2, lty = 3)\n\n# Legend\nlegend(\"topleft\", legend = c(\"Actual counts\", \"Random Forest (ntree = 500)\", \"XGBoost (nrounds = 50)\"), \n       col = c(\"green\", \"cornflowerblue\", \"lightcoral\"), lty = c(1, 2, 3), lwd = 2, cex = 0.7)\n\n\n\n\n\n\n\n\nWe can appreciate how XGBoost seems to handle better the dynamic changes in the number of rentals, particularly during rapid increases and decreases, but overestimating the peaks. In the other hand, random forest performs more conservatively but struggles with capturing sharp increases (the averaging of trees reduces variance but introduce more bias)."
  },
  {
    "objectID": "index.html#bagging-and-boosting-for-bike-rental-data-regression",
    "href": "index.html#bagging-and-boosting-for-bike-rental-data-regression",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "",
    "text": "Use the randomForest() function in the randomForest package to fit three random forest regressions (bagging algorithms) with the ntree argument set to, respectively, 50, 100, and 500 (number of trees to grow). In each case, compute the root mean squared error (RMSE) for the training and test datasets.\n\nThe following code prepares the dataset and sets the training and testing data:\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\nsuppressMessages(library(dplyr))\nsuppressMessages(library(splines))\nbike_data &lt;- read.csv('/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 2/bike_rental_hourly.csv')\nbike_data$log_cnt &lt;- log(bike_data$cnt)\nbike_data$hour &lt;- bike_data$hr/23 # transform [0, 23] to [0, 1]. 0 is midnight, 1 is 11 PM\n\n# One hot for weathersit\none_hot_encode_weathersit &lt;- model.matrix(~ as.factor(weathersit) - 1,data = bike_data)\none_hot_encode_weathersit  &lt;- one_hot_encode_weathersit[, -1] # Remove reference category\ncolnames(one_hot_encode_weathersit) &lt;- c('cloudy', 'light rain', 'heavy rain')\nbike_data &lt;- cbind(bike_data, one_hot_encode_weathersit)\n\n# One hot for weekday\none_hot_encode_weekday &lt;- model.matrix(~ as.factor(weekday) - 1,data = bike_data)\none_hot_encode_weekday  &lt;- one_hot_encode_weekday[, -1] # Remove reference category\ncolnames(one_hot_encode_weekday) &lt;- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')\nbike_data &lt;- cbind(bike_data, one_hot_encode_weekday)\n\n# One hot for weekday\none_hot_encode_season &lt;- model.matrix(~ as.factor(season) - 1,data = bike_data)\none_hot_encode_season  &lt;- one_hot_encode_season[, -1] # Remove reference category\ncolnames(one_hot_encode_season) &lt;- c('Spring', 'Summer', 'Fall')\nbike_data &lt;- cbind(bike_data, one_hot_encode_season)\n\n# Create lags\nbike_data_new &lt;- mutate(bike_data, lag1 = lag(log_cnt, 1), lag2 = lag(log_cnt, 2), \n                        lag3 = lag(log_cnt, 3), lag4 = lag(log_cnt, 4), lag24 = lag(log_cnt, 24))\n\nbike_data_new &lt;- bike_data_new[-c(1:24),] # Lost 24 obs because of lagging\n\n# Create training and test data\nbike_all_data_train &lt;- bike_data_new[bike_data_new$dteday &gt;= as.Date(\"2011-01-01\") & bike_data_new$dteday &lt;=  as.Date(\"2012-05-31\"), ]\nbike_all_data_test &lt;- bike_data_new[bike_data_new$dteday &gt;= as.Date(\"2012-06-01\") & bike_data_new$dteday &lt;=  as.Date(\"2012-12-31\"), ]\nX_train &lt;- cbind(1, bike_all_data_train[, c(\"lag1\", \"lag2\",  \"lag3\", \"lag4\", \"lag24\")])\nspline_basis &lt;- ns(bike_all_data_train$hour, df = 10, intercept = FALSE)\nX_train &lt;- cbind(X_train, spline_basis)\ncolnames(X_train)[1] &lt;- \"intercept\"\nknots &lt;- attr(spline_basis, \"knots\")\nvariables_to_keep_in_X &lt;- c(\"yr\", \"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\") \nvariables_to_keep_in_X &lt;- c(variables_to_keep_in_X, colnames(one_hot_encode_weathersit), colnames(one_hot_encode_weekday), colnames(one_hot_encode_season))\nX_train &lt;- cbind(X_train, bike_all_data_train[, variables_to_keep_in_X])\n# Training data\nX_train &lt;- as.matrix(X_train)\ny_train &lt;- bike_all_data_train$log_cnt\n# Test data\ny_test &lt;- bike_all_data_test$log_cnt\nX_test &lt;- cbind(1, bike_all_data_test[, c(\"lag1\", \"lag2\",  \"lag3\", \"lag4\", \"lag24\")])\nspline_basis_test &lt;- ns(bike_all_data_test$hour, df=10, knots=knots, intercept = FALSE)\nX_test &lt;- cbind(X_test, spline_basis_test)\ncolnames(X_test)[1] &lt;- \"intercept\"\nX_test &lt;- cbind(X_test, bike_all_data_test[, variables_to_keep_in_X])\nX_test &lt;- as.matrix(X_test)\n\nThe following code uses the randomForest() function in the randomForest package to fit three random forest regressions (bagging algorithms) with the ntree argument set to, respectively, 50, 100, and 500 (number of trees to grow). Then it computes the RMSE for the training and test datasets:\n\n# Load library\nsuppressMessages(library(randomForest))\n\n# Random Forest with ntree = 50\nset.seed(123)\nForest_cincuenta &lt;- randomForest(X_train, y_train, ntree = 50)\ny_train_cincuenta &lt;- predict(Forest_cincuenta, X_train)\ny_test_cincuenta &lt;- predict(Forest_cincuenta, X_test)\n\n# RMSE for ntree = 50\nrmse_train_cincuenta &lt;- sqrt(mean((y_train - y_train_cincuenta)^2))\nrmse_test_cincuenta &lt;- sqrt(mean((y_test - y_test_cincuenta)^2))\n\n# Random Forest with ntree = 100\nForest_cien &lt;- randomForest(X_train, y_train, ntree = 100)\ny_train_cien &lt;- predict(Forest_cien, X_train)\ny_test_cien &lt;- predict(Forest_cien, X_test)\n\n# RMSE for ntree = 100\nrmse_train_cien &lt;- sqrt(mean((y_train - y_train_cien)^2))\nrmse_test_cien &lt;- sqrt(mean((y_test - y_test_cien)^2))\n\n# Random Forest with ntree = 500\nForest_quinientos &lt;- randomForest(X_train, y_train, ntree = 500)\ny_train_quinientos &lt;- predict(Forest_quinientos, X_train)\ny_test_quinientos &lt;- predict(Forest_quinientos, X_test)\n\n# RMSE for ntree = 500\nrmse_train_quinientos &lt;- sqrt(mean((y_train - y_train_quinientos)^2))\nrmse_test_quinientos &lt;- sqrt(mean((y_test - y_test_quinientos)^2))\n\n# Print RMSE values\nprint(paste(\"RMSE for ntree = 50 on the training data:\", rmse_train_cincuenta))\n\n[1] \"RMSE for ntree = 50 on the training data: 0.143799442480079\"\n\nprint(paste(\"RMSE for ntree = 50 on the test data:\", rmse_test_cincuenta))\n\n[1] \"RMSE for ntree = 50 on the test data: 0.295419159165542\"\n\nprint(paste(\"RMSE for ntree = 100 on the training data:\", rmse_train_cien))\n\n[1] \"RMSE for ntree = 100 on the training data: 0.141731736751503\"\n\nprint(paste(\"RMSE for ntree = 100 on the test data:\", rmse_test_cien))\n\n[1] \"RMSE for ntree = 100 on the test data: 0.292150287649632\"\n\nprint(paste(\"RMSE for ntree = 500 on the training data:\", rmse_train_quinientos))\n\n[1] \"RMSE for ntree = 500 on the training data: 0.139092722085123\"\n\nprint(paste(\"RMSE for ntree = 500 on the test data:\", rmse_test_quinientos))\n\n[1] \"RMSE for ntree = 500 on the test data: 0.290405402095089\"\n\n\n\n\n\n\nPlot a time series plot of the response in the original scale (i.e.Â counts and not log-counts) for the last week of the test data (last \\(24\\times 7\\) observations). In the same figure, plot a time series plot of the fitted values (in the original scale) from Problem 1.1 when using ntree=50 and ntree=100. Comment on the results.\n\nThe next code plots a time series plot of the response in the original scale for the last week of the test data, in the same figure, plots a time series plot of the fitted values (in the original scale) from Problem 1.1 when using ntree=50 and ntree=100.:\n\n# Llast week test data\nlast_week_test &lt;- tail(bike_all_data_test, 24 * 7)\n\n# Predicted values for ntree = 50 and ntree = 100 in original scale\ny_test_pred_cincuenta_org &lt;- exp(y_test_cincuenta)\ny_test_pred_cien_org &lt;- exp(y_test_cien)\n\n# Last week of predicted values\nlast_week_pred_cincuenta &lt;- tail(y_test_pred_cincuenta_org, 24 * 7)\nlast_week_pred_cien &lt;- tail(y_test_pred_cien_org, 24 * 7)\n\n# Last week bike counts in the original scale\nlast_week_y_test &lt;- tail(exp(y_test), 24 * 7)\n\n# Plot\nplot(last_week_y_test, type = \"l\", col = \"green\", lwd = 2, xlab = \"Time in hours\", ylab = \"Number of bike rentals\", main = \"Time Series Plot of Actual and Predicted Bike Rentals for the Last Week\", cex.main = 0.8)\nlines(last_week_pred_cincuenta, col = \"cornflowerblue\", lwd = 2, lty = 2)\nlines(last_week_pred_cien, col = \"lightcoral\", lwd = 2, lty = 3)\n\n# Legend\nlegend(\"topleft\", legend = c(\"Actual counts\", \"Predicted (ntree = 50)\", \"Predicted (ntree = 100)\"), col = c(\"green\", \"cornflowerblue\", \"lightcoral\"), lty = c(1, 2, 3), lwd = 2, cex = 0.8)\n\n\n\n\n\n\n\n\nAs we can see in the chart, when we use a larger number of trees to grow, the predicted number of bike rides approximates better the actual number of bike rides in the peaks. This happens because more trees allow for better generalization and lower variance in the predictions, which results in the model capturing the peaks more accurately.\n\n\n\n\nUse the xgboost() function in the xgboost package to fit three extreme gradient boosting regressions (boosting algorithms) with the nrounds argument set to, respectively, 10, 25, and 50 (max number of boosting iterations). In each case, compute the root mean squared error (RMSE) for the training and test datasets.\n\nThe next code uses the xgboost() function in the xgboost package to fit three extreme gradient boosting regressions with the nrounds argument set to, respectively, 10, 25, and 50. Moreover, the code computes and prints the root mean squared error for the training and test datasets:\n\nsuppressMessages(library(xgboost))\n\n# Convert the data to a compact format for xgboost, we use y_train as label\nxgtrain &lt;- xgb.DMatrix(data = X_train, label = y_train)\n\n# Using xgboost with 10 iterations using a regression with squared loss\nxgb_diez &lt;- xgboost(data = xgtrain, nrounds = 10, objective = \"reg:squarederror\", verbose = 0)\n\n# Predict\ny_train_xgb_diez &lt;- predict(xgb_diez, newdata = X_train)\ny_test_xgb_diez &lt;- predict(xgb_diez, newdata = X_test)\n\n# Using xgboost with 25 iterations using a regression with squared loss\nxgb_venticinco &lt;- xgboost(data = xgtrain, nrounds = 25, objective = \"reg:squarederror\", verbose = 0)\n\n# Predict\ny_train_xgb_venticinco &lt;- predict(xgb_venticinco, newdata = X_train)\ny_test_xgb_venticinco &lt;- predict(xgb_venticinco, newdata = X_test)\n\n# Using xgboost with 50 iterations using a regression with squared loss\nxgb_cincuenta &lt;- xgboost(data = xgtrain, nrounds = 50, objective = \"reg:squarederror\", verbose = 0)\n\n# Predictions for training and test data\ny_train_xgb_cincuenta &lt;- predict(xgb_cincuenta, newdata = X_train)\ny_test_xgb_cincuenta &lt;- predict(xgb_cincuenta, newdata = X_test)\n\n# Calculating and printing RMSE values\nrmse_train_diez_xgb &lt;- sqrt(mean((y_train - y_train_xgb_diez)^2))\nrmse_test_diez_xgb &lt;- sqrt(mean((y_test - y_test_xgb_diez)^2))\nrmse_train_venticinco_xgb &lt;- sqrt(mean((y_train - y_train_xgb_venticinco)^2))\nrmse_test_venticinco_xgb &lt;- sqrt(mean((y_test - y_test_xgb_venticinco)^2))\nrmse_train_cincuenta_xgb &lt;- sqrt(mean((y_train - y_train_xgb_cincuenta)^2))\nrmse_test_cincuenta_xgb &lt;- sqrt(mean((y_test - y_test_xgb_cincuenta)^2))\nprint(paste(\"RMSE for nrounds = 10 on the training data:\", rmse_train_diez_xgb))\n\n[1] \"RMSE for nrounds = 10 on the training data: 0.323424139233079\"\n\nprint(paste(\"RMSE for nrounds = 10 on the test data:\", rmse_test_diez_xgb))\n\n[1] \"RMSE for nrounds = 10 on the test data: 0.367012312415504\"\n\nprint(paste(\"RMSE for nrounds = 25 on the training data:\", rmse_train_venticinco_xgb))\n\n[1] \"RMSE for nrounds = 25 on the training data: 0.254995729934359\"\n\nprint(paste(\"RMSE for nrounds = 25 on the test data:\", rmse_test_venticinco_xgb))\n\n[1] \"RMSE for nrounds = 25 on the test data: 0.302291709129745\"\n\nprint(paste(\"RMSE for nrounds = 50 on the training data:\", rmse_train_cincuenta_xgb))\n\n[1] \"RMSE for nrounds = 50 on the training data: 0.215796470201241\"\n\nprint(paste(\"RMSE for nrounds = 50 on the test data:\", rmse_test_cincuenta_xgb))\n\n[1] \"RMSE for nrounds = 50 on the test data: 0.300357225012064\"\n\n\n\n\n\n\nPlot the response in the original scale for the last week of the test data (last \\(24\\times 7\\) observations) vs the corresponding fitted values from the best bagging model in Problem 1.2 and the best boosting model in Problem 1.3. Comment on the results.\n\nThe next code plots the response in the original scale for the last week of the test data vs the corresponding fitted values from the best bagging model in Problem 1.2 (ntree =500, lowest RMSE on test data) and the best boosting model in Problem 1.3 (nrounds =50, lowest RMSE on test data):\n\n# Predicted values in original scale\nrf_top_original &lt;- exp(y_test_quinientos)\nxgb_top_original &lt;- exp(y_test_xgb_cincuenta)\n\n# Last week of predicted values for each model\nlast_week_rf_top_original &lt;- tail(rf_top_original, 24 * 7)\nlast_week_xgb_top_original &lt;- tail(xgb_top_original, 24 * 7)\n\n# Plot\nplot(last_week_y_test, type = \"l\", col = \"green\", lwd = 2, xlab = \"Time in hours\", ylab = \"Number of Bike Rentals\", \n     main = \"Actual vs Predicted Bike Rentals for the Last Week\", cex.main = 0.8, ylim = range(c(last_week_y_test, last_week_rf_top_original, last_week_xgb_top_original)))\nlines(last_week_rf_top_original, col = \"cornflowerblue\", lwd = 2, lty = 2)\nlines(last_week_xgb_top_original, col = \"lightcoral\", lwd = 2, lty = 3)\n\n# Legend\nlegend(\"topleft\", legend = c(\"Actual counts\", \"Random Forest (ntree = 500)\", \"XGBoost (nrounds = 50)\"), \n       col = c(\"green\", \"cornflowerblue\", \"lightcoral\"), lty = c(1, 2, 3), lwd = 2, cex = 0.7)\n\n\n\n\n\n\n\n\nWe can appreciate how XGBoost seems to handle better the dynamic changes in the number of rentals, particularly during rapid increases and decreases, but overestimating the peaks. In the other hand, random forest performs more conservatively but struggles with capturing sharp increases (the averaging of trees reduces variance but introduce more bias)."
  },
  {
    "objectID": "index.html#bagging-and-boosting-for-spam-email-data-classification",
    "href": "index.html#bagging-and-boosting-for-spam-email-data-classification",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "2. Bagging and boosting for spam email data (classification)",
    "text": "2. Bagging and boosting for spam email data (classification)\n\nð¾ Problem 2.1\n\nUse the randomForest() function in the randomForest package to fit three random forest classification with the ntree argument set to, respectively, 50, 100, and 500. In each case, plot the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package pROC() (all plots in the same figure). Comment on the results.\n\nThe following code prepares the dataset and sets the training and testing data:\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\nload(file = '/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 2/spam_ham_emails.RData')\nSpam_ham_emails[, -1] &lt;- scale(Spam_ham_emails[, -1])\nSpam_ham_emails['spam'] &lt;- as.factor(Spam_ham_emails['spam'] == 1) # Changing from 1-&gt;TRUE, 0-&gt;FALSE\nlevels(Spam_ham_emails$spam) &lt;- c(\"not spam\", \"spam\")\nsuppressMessages(library(caret))\ntrain_obs &lt;- createDataPartition(y = Spam_ham_emails$spam, p = .75, list = FALSE)\ntrain &lt;- Spam_ham_emails[train_obs, ]\ny_train &lt;- train$spam\nX_train &lt;- train[, -1]\ntest &lt;- Spam_ham_emails[-train_obs, ]\ny_test &lt;- test$spam\nX_test &lt;- test[, -1]\n\n# Confirm both training and test are balanced with respect to spam emails\ncat(\"Percentage of training data consisting of spam emails:\", \n              100*mean(train$spam == \"spam\"))\n\nPercentage of training data consisting of spam emails: 39.40887\n\ncat(\"Percentage of test data consisting of spam emails:\", \n              100*mean(test$spam == \"spam\"))\n\nPercentage of test data consisting of spam emails: 39.3913\n\n\nThe next code uses the randomForest() function in the randomForest package to fit three random forest classification with the ntree argument set to, respectively, 50, 100, and 500. Moreover, the code plots the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package pROC():\n\nsuppressMessages(library(pROC))\nsuppressMessages(library(randomForest))\n# Train 3 diffrent random forest models wites varing ntree values\nrf_50 &lt;- randomForest(spam ~ ., data=train, importance=TRUE,proximity=TRUE,ntree=50)\nrf_100 &lt;- randomForest(spam ~ ., data=train, importance=TRUE,proximity=TRUE,ntree=100)\nrf_500 &lt;- randomForest(spam ~ ., data=train, importance=TRUE,proximity=TRUE,ntree=500)\n\n# Generate predictions for different random forest models with varying ntree values\nprob_rf_50 &lt;- predict(rf_50, test, type = \"prob\")[, \"spam\"]\nprob_rf_100 &lt;- predict(rf_100, test, type = \"prob\")[, \"spam\"]\nprob_rf_500 &lt;- predict(rf_500, test, type = \"prob\")[, \"spam\"]\n\n# Compute ROC curves for each model\nroc_rf_50 &lt;- roc(y_test, prob_rf_50, levels = c(\"not spam\", \"spam\"), direction = \"&lt;\")\nroc_rf_100 &lt;- roc(y_test, prob_rf_100, levels = c(\"not spam\", \"spam\"), direction = \"&lt;\")\nroc_rf_500 &lt;- roc(y_test, prob_rf_500, levels = c(\"not spam\", \"spam\"), direction = \"&lt;\")\n\n# Plot the ROC curves in the same plot\nplot(roc_rf_50, col = \"cornflowerblue\", main = \"ROC Curves for Random Forest Models with Different ntree Values\")\nlines(roc_rf_100, col = \"mediumseagreen\")\nlines(roc_rf_500, col = \"lightcoral\")\nlegend(\"bottomright\", legend = c(\"ntree = 50\", \"ntree = 100\", \"ntree = 500\"), col = c(\"cornflowerblue\", \"mediumseagreen\", \"lightcoral\"), lwd = 1)\n\n\n\n\n\n\n\n# Compute AUC for each model\nauc_rf_50 &lt;- auc(roc_rf_50)\nauc_rf_100 &lt;- auc(roc_rf_100)\nauc_rf_500 &lt;- auc(roc_rf_500)\n\n# Print AUC values for each model\ncat(\"AUC for ntree = 50:\", auc_rf_50, \"\\n\")\n\nAUC for ntree = 50: 0.9794278 \n\ncat(\"AUC for ntree = 100:\", auc_rf_100, \"\\n\")\n\nAUC for ntree = 100: 0.9788878 \n\ncat(\"AUC for ntree = 500:\", auc_rf_500, \"\\n\")\n\nAUC for ntree = 500: 0.9798902 \n\n\nThe AUC difference between ntree = 50 and ntree = 500 is less than 0.001, so using fewer trees is more computationally efficient, as it provides almost the same classification performance than using many trees, but with less computational overhead.\nAs the ROC curves show, increasing the number of trees from 50 to 500 does not significantly change the curve, so adding more trees doesnât bring added benefit in terms of ROC performance.\n\n\nð¾ Problem 2.2\n\nPredict the test data using the random forest classifier in Problem 2.1 with ntree=100 following the rule: if \\(\\mathrm{Pr}(y=1|\\mathbf{x})&gt;0.5 \\Rightarrow y=1\\), and compute the confusion matrix using the confusionMatrix() function from the caret package.\n\nThe next code predicts the test data using the random forest classifier in Problem 2.1 with ntree=100 following the rule: if \\(\\mathrm{Pr}(y=1|\\mathbf{x})&gt;0.5 \\Rightarrow y=1\\), it also computes the confusion matrix using the confusionMatrix() function from the caret package.\n\nthreshold &lt;- 0.5 # Predict spam if probability &gt; threshold\ny_rf_100 &lt;- as.factor(prob_rf_100 &gt; threshold) \nlevels(y_rf_100) &lt;- c(\"not spam\", \"spam\")\n\n##  confusionMatrix for ntree=100\nconfusionMatrix(data = y_rf_100, y_test, positive = \"spam\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction not spam spam\n  not spam      666   36\n  spam           31  417\n                                          \n               Accuracy : 0.9417          \n                 95% CI : (0.9266, 0.9546)\n    No Information Rate : 0.6061          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8777          \n                                          \n Mcnemar's Test P-Value : 0.6251          \n                                          \n            Sensitivity : 0.9205          \n            Specificity : 0.9555          \n         Pos Pred Value : 0.9308          \n         Neg Pred Value : 0.9487          \n             Prevalence : 0.3939          \n         Detection Rate : 0.3626          \n   Detection Prevalence : 0.3896          \n      Balanced Accuracy : 0.9380          \n                                          \n       'Positive' Class : spam            \n                                          \n\n\n\n\nð¾ Problem 2.3\n\nUse the xgboost() function in the xgboost package to fit three extreme gradient boosting regressions with the nrounds argument set to, respectively, 10, 25, and 50. In each case, plot the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package pROC() (all plots in the same figure). Moreover, add the ROC plot for the random forest in Problem 2.1 with ntree=100. Comment on the results.\n\nThe next code uses the xgboost() function in the xgboost package to fit three extreme gradient boosting regressions with the nrounds argument set to, respectively, 10, 25, and 50. In each case,it plots the ROC curve with the ROC curve of the random forest of Problem 2.1 and computes the area under the ROC curves (AUC) for the test data using the package pROC().\n\nX_train_xgb &lt;- as.matrix(X_train)\nX_test_xgb &lt;- as.matrix(X_test)\ny_train_xgb &lt;- as.integer(y_train) - 1\ny_test_xgb &lt;- as.integer(y_test) - 1\n\nlibrary(xgboost)\nxg_10 &lt;- xgboost(data = X_train_xgb, label = y_train_xgb, max.depth = 2, eta = 1, nthread = 2, nrounds = 10, objective = \"binary:logistic\")\n\n[1] train-logloss:0.408653 \n[2] train-logloss:0.307379 \n[3] train-logloss:0.257045 \n[4] train-logloss:0.223842 \n[5] train-logloss:0.197048 \n[6] train-logloss:0.186351 \n[7] train-logloss:0.178877 \n[8] train-logloss:0.173966 \n[9] train-logloss:0.169304 \n[10]    train-logloss:0.164715 \n\nxg_25 &lt;- xgboost(data = X_train_xgb, label = y_train_xgb, max.depth = 2, eta = 1, nthread = 2, nrounds = 25, objective = \"binary:logistic\")\n\n[1] train-logloss:0.408653 \n[2] train-logloss:0.307379 \n[3] train-logloss:0.257045 \n[4] train-logloss:0.223842 \n[5] train-logloss:0.197048 \n[6] train-logloss:0.186351 \n[7] train-logloss:0.178877 \n[8] train-logloss:0.173966 \n[9] train-logloss:0.169304 \n[10]    train-logloss:0.164715 \n[11]    train-logloss:0.161381 \n[12]    train-logloss:0.156250 \n[13]    train-logloss:0.153289 \n[14]    train-logloss:0.150130 \n[15]    train-logloss:0.148277 \n[16]    train-logloss:0.146104 \n[17]    train-logloss:0.144178 \n[18]    train-logloss:0.142540 \n[19]    train-logloss:0.141497 \n[20]    train-logloss:0.140479 \n[21]    train-logloss:0.138079 \n[22]    train-logloss:0.136144 \n[23]    train-logloss:0.134225 \n[24]    train-logloss:0.132321 \n[25]    train-logloss:0.130088 \n\nxg_50 &lt;- xgboost(data = X_train_xgb, label = y_train_xgb, max.depth = 2, eta = 1, nthread = 2, nrounds = 50, objective = \"binary:logistic\")\n\n[1] train-logloss:0.408653 \n[2] train-logloss:0.307379 \n[3] train-logloss:0.257045 \n[4] train-logloss:0.223842 \n[5] train-logloss:0.197048 \n[6] train-logloss:0.186351 \n[7] train-logloss:0.178877 \n[8] train-logloss:0.173966 \n[9] train-logloss:0.169304 \n[10]    train-logloss:0.164715 \n[11]    train-logloss:0.161381 \n[12]    train-logloss:0.156250 \n[13]    train-logloss:0.153289 \n[14]    train-logloss:0.150130 \n[15]    train-logloss:0.148277 \n[16]    train-logloss:0.146104 \n[17]    train-logloss:0.144178 \n[18]    train-logloss:0.142540 \n[19]    train-logloss:0.141497 \n[20]    train-logloss:0.140479 \n[21]    train-logloss:0.138079 \n[22]    train-logloss:0.136144 \n[23]    train-logloss:0.134225 \n[24]    train-logloss:0.132321 \n[25]    train-logloss:0.130088 \n[26]    train-logloss:0.128563 \n[27]    train-logloss:0.127664 \n[28]    train-logloss:0.126233 \n[29]    train-logloss:0.125122 \n[30]    train-logloss:0.123357 \n[31]    train-logloss:0.122754 \n[32]    train-logloss:0.121732 \n[33]    train-logloss:0.120408 \n[34]    train-logloss:0.119414 \n[35]    train-logloss:0.117893 \n[36]    train-logloss:0.117279 \n[37]    train-logloss:0.116227 \n[38]    train-logloss:0.115185 \n[39]    train-logloss:0.114325 \n[40]    train-logloss:0.112993 \n[41]    train-logloss:0.111710 \n[42]    train-logloss:0.111270 \n[43]    train-logloss:0.110123 \n[44]    train-logloss:0.109701 \n[45]    train-logloss:0.109245 \n[46]    train-logloss:0.108495 \n[47]    train-logloss:0.107198 \n[48]    train-logloss:0.106384 \n[49]    train-logloss:0.105464 \n[50]    train-logloss:0.104922 \n\n# Caluculate the pobability\nxg_prob_10 &lt;- predict(xg_10, X_test_xgb)\nxg_prob_25 &lt;- predict(xg_25, X_test_xgb)\nxg_prob_50 &lt;- predict(xg_50, X_test_xgb)\n\n# Compute ROC curves for each model\nroc_xg_10 &lt;- roc(y_test, xg_prob_10, levels = c(\"not spam\", \"spam\"), direction = \"&lt;\")\nroc_xg_25 &lt;- roc(y_test, xg_prob_25, levels = c(\"not spam\", \"spam\"), direction = \"&lt;\")\nroc_xg_50 &lt;- roc(y_test, xg_prob_50, levels = c(\"not spam\", \"spam\"), direction = \"&lt;\")\n\n# Plot the ROC curves \nplot(roc_xg_10, col = \"cornflowerblue\", main = \"ROC Curves for XGboost Models with Different ntree Values\")\nlines(roc_xg_25, col = \"mediumseagreen\")\nlines(roc_xg_50, col = \"lightcoral\")\nlines(roc_rf_100, col = \"yellow\")\nlegend(\"bottomright\", legend = c(\"nrounds = 10\", \"nrounds = 25\", \"nrounds = 50\",\"ntree = 100\"), col = c(\"cornflowerblue\", \"mediumseagreen\", \"lightcoral\",'yellow'), lwd = 1)\n\n\n\n\n\n\n\n# Compute AUC for each model\nauc_xg_10 &lt;- auc(roc_xg_10)\nauc_xg_25 &lt;- auc(roc_xg_25)\nauc_xg_50 &lt;- auc(roc_xg_50)\n\n# Print AUC values for each model\ncat(\"AUC for nrounds = 10:\", auc_xg_10, \"\\n\")\n\nAUC for nrounds = 10: 0.9744664 \n\ncat(\"AUC for nrounds = 25:\", auc_xg_25, \"\\n\")\n\nAUC for nrounds = 25: 0.9754435 \n\ncat(\"AUC for nrounds = 50:\", auc_xg_50, \"\\n\")\n\nAUC for nrounds = 50: 0.9744791 \n\n\nSimilar to the behavior observed in Random Forest models, increasing nrounds in XGBoost shows diminishing returns. While boosting iterations can improve the modelâs learning capacity, after a certain point, the AUC reaches a plateau, and further increases in nrounds provide minimum improvements, so it would be more computationally efficient to use fewer rounds.\n\n\nð¾ Problem 2.4\n\nPredict the test data using the extreme gradient boosting classifier in Problem 2.3 with nrounds=25 following the rule: if \\(\\mathrm{Pr}(y=1|\\mathbf{x})&gt;0.5 \\Rightarrow y=1\\), and compute the confusion matrix using the confusionMatrix() function from the caret package.\n\nThe next code predicts the test data using the extreme gradient boosting classifier in Problem 2.3 with nrounds=25 following the rule: if \\(\\mathrm{Pr}(y=1|\\mathbf{x})&gt;0.5 \\Rightarrow y=1\\), and computes the confusion matrix using the confusionMatrix() function from the caret package.\n\nthreshold &lt;- 0.5 # Predict spam if probability &gt; 50%\ny_xg_25 &lt;- as.factor(xg_prob_25 &gt; threshold) \nlevels(y_xg_25) &lt;- c(\"not spam\", \"spam\")\n\n##  confusionMatrix for ntree=100\nconfusionMatrix(data = y_xg_25, y_test, positive = \"spam\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction not spam spam\n  not spam      664   35\n  spam           33  418\n                                          \n               Accuracy : 0.9409          \n                 95% CI : (0.9256, 0.9538)\n    No Information Rate : 0.6061          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8761          \n                                          \n Mcnemar's Test P-Value : 0.9035          \n                                          \n            Sensitivity : 0.9227          \n            Specificity : 0.9527          \n         Pos Pred Value : 0.9268          \n         Neg Pred Value : 0.9499          \n             Prevalence : 0.3939          \n         Detection Rate : 0.3635          \n   Detection Prevalence : 0.3922          \n      Balanced Accuracy : 0.9377          \n                                          \n       'Positive' Class : spam"
  },
  {
    "objectID": "index.html#learning-parametric-models-by-gradient-based-optimisation",
    "href": "index.html#learning-parametric-models-by-gradient-based-optimisation",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "3. Learning parametric models by gradient based optimisation",
    "text": "3. Learning parametric models by gradient based optimisation\n\nð¾ Problem 3.1\n\nSimulate \\(n=1000\\) independent (conditionally on \\(x\\)) \\(y\\) observations from the Poisson regression model \\[y|\\beta_0,\\beta_1 \\sim \\mathrm{Poisson}(\\exp\\left(\\beta_0 + \\beta_1x \\right)),\\] with the true parameters \\(\\beta_0= 0.3\\) and \\(\\beta_1= 2.5\\).\n\nThe next code simulates \\(n=1000\\) independent (conditionally on \\(x\\)), \\(y\\) observations from the Poisson regression model \\[y|\\beta_0,\\beta_1 \\sim \\mathrm{Poisson}(\\exp\\left(\\beta_0 + \\beta_1x \\right)),\\] with the true parameters \\(\\beta_0= 0.3\\) and \\(\\beta_1= 2.5\\).\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\ngen_x &lt;- function(seed=123){\n  set.seed(seed)\n  return(runif(0, 1, n = 1000))\n}\n# Test\nx &lt;- gen_x()\n\n# Parameters\nb_0 &lt;- 0.3\nb_1 &lt;- 2.5\n\n# Calculate the Poisson rate (lambda) using the true model parameters\nlambda &lt;- exp(b_0 + b_1 * x)\n\n# Simulate y from a Poisson distribution with rate lambda\ny &lt;- rpois(length(x), lambda) \n\n\n\nð¾ Problem 3.2\n\nIn the same figure, plot the scatter \\(\\{x_i, y_i\\}_{i=1}^n\\) and the conditional expected value \\(\\mathbb{E}(y|x)\\) as a function of \\(x\\) given the true parameter values.\n\nThe next code plots the scatter \\(\\{x_i, y_i\\}_{i=1}^n\\) and the conditional expected value \\(\\mathbb{E}(y|x)\\) as a function of \\(x\\) given the true parameter values.\n\n# Scatter plot of the observed x and y values\nplot(x, y, main = \"Scatter plot of x and y with Conditional Expected Value\",\n     xlab = \"x\", ylab = \"y\", pch = 20, col = \"cornflowerblue\", cex = 0.5)\n\n# Create x_grid for evaluating the conditional expected value\nx_grid &lt;- seq(0, 1, length.out = 10000)\n\n# Calculate the conditional expected value E(y|x) = exp(b_0 + b_1 * x)\nexpected_y &lt;- exp(b_0 + b_1 * x_grid)\n\n# Add the expected value curve to the plot\nlines(x_grid, expected_y, col = \"lightcoral\", lwd = 2)\n\n# Add a legend\nlegend(\"topleft\", legend = c(\"Observed y\", \"E(y|x)\"), \n       col = c(\"cornflowerblue\", \"lightcoral\"), pch = c(20, NA), lty = c(NA, 1), lwd = c(NA, 2))\n\n\n\n\n\n\n\n\n\n\nð¾ Problem 3.3\n\nDerive (analytically) the log-likelihood \\(\\ell(\\beta_0, \\beta_1)\\) for the Poisson regression model.\n\n\nProbability mass function\n\nThe probability mass function of a Poisson distribution is given by:\n\\[\np(y_i|\\mu_i) = \\frac{\\mu_i^{y_i} \\exp(-\\mu_i)}{y_i!}\n\\]\nwhere \\(\\mu_i\\) is the mean for each \\(y_i\\).\nIn a Poisson regression model, the expected value \\(\\mu_i\\) is given by:\n\\[\n\\mu_i = \\exp(\\beta_0 + \\beta_1 x_i)\n\\]\nThen, we can write the probability mass function of \\(y_i\\), conditional on \\(x_i\\) as:\n\\[\np(y_i | x_i, \\beta_0, \\beta_1) = \\frac{\\left(\\exp(\\beta_0 + \\beta_1 x_i)\\right)^{y_i} \\exp\\left(-\\exp(\\beta_0 + \\beta_1 x_i)\\right)}{y_i!}\n\\]\n\nLog-likelihood function\n\nThe log-likelihood function for the entire dataset is the log of the joint probability of observing all \\(y_i\\)âs, assuming independence of the \\(y_i\\)âs, conditional on the \\(x_i\\)âs.\nThus, the log-likelihood \\(\\ell(\\beta_0, \\beta_1)\\) is:\n\\[\n\\ell(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\log \\left( p(y_i | x_i, \\beta_0, \\beta_1) \\right)\n\\]\nSubstituting the probability mass function of the Poisson distribution into \\(\\ell(\\beta_0, \\beta_1)\\), we get:\n\\[\n\\ell(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\log \\left( \\frac{\\left(\\exp(\\beta_0 + \\beta_1 x_i)\\right)^{y_i} \\exp\\left(-\\exp(\\beta_0 + \\beta_1 x_i)\\right)}{y_i!} \\right)\n\\]\nUsing the properties of the logarithms we can simplify \\(\\ell(\\beta_0, \\beta_1)\\) as:\n\\[\n\\ell(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\left( y_i (\\beta_0 + \\beta_1 x_i) - \\exp(\\beta_0 + \\beta_1 x_i) - \\log(y_i!) \\right)\n\\]\nThis log-likelihood function can be maximized with respect to \\(\\beta_0\\) and \\(\\beta_1\\) to estimate the parameters using numerical optimization methods, such as gradient descent or other gradient-based optimizers.\n\n\nð¾ Problem 3.4\n\nDerive (analytically) the gradient of the log-likelihood \\(\\ell(\\beta_0, \\beta_1)\\) for the Poisson regression model.\n\nWe know that because of the (conditional) independence assumption, \\(\\nabla \\ell(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\nabla \\log p(y_i | x_i, \\beta_0, \\beta_1)\\) and noting that \\(\\nabla \\log p(y_i|x_i,\\beta_0, \\beta_1)=\\left(\\frac{\\partial}{\\partial\\beta_0}\\log p(y_i|x_i,\\beta_0, \\beta_1), \\frac{\\partial}{\\partial\\beta_1}\\log p(y_i|x_i,\\beta_0, \\beta_1) \\right)^\\top.\\) Then we can find \\(\\nabla \\ell(\\beta_0, \\beta_1)\\):\nThe log of the Poisson probability mass function is: \\(\\log p(y_i | x_i, \\beta_0, \\beta_1) = y_i (\\beta_0 + \\beta_1 x_i) - \\exp(\\beta_0 + \\beta_1 x_i) - \\log(y_i!)\\), then we can calculate the gradient respect to to each \\(\\beta\\):\nThe gradient with respect to \\(\\beta_0\\):\n\\[\\frac{\\partial}{\\partial \\beta_0} \\log p(y_i | x_i, \\beta_0, \\beta_1) = y_i - \\exp(\\beta_0 + \\beta_1 x_i)\\]\nThe total gradient with respect to \\(\\beta_0\\):\n\\[\\frac{\\partial \\ell(\\beta_0, \\beta_1)}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\left( y_i - \\exp(\\beta_0 + \\beta_1 x_i) \\right)\\]\nThe gradient with respect to \\(\\beta_1\\):\n\\[\\frac{\\partial}{\\partial \\beta_1} \\log p(y_i | x_i, \\beta_0, \\beta_1) = x_i \\left( y_i - \\exp(\\beta_0 + \\beta_1 x_i) \\right) \\]\nThe total gradient with respect to \\(\\beta_1\\):\n\\[\\frac{\\partial \\ell(\\beta_0, \\beta_1)}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\left(x_i(y_i - \\exp(\\beta_0 + \\beta_1 x_i))  \\right)\\]\nNow we can re-write \\(\\nabla \\ell(\\beta_0, \\beta_1)\\) as:\n\\[\\nabla \\ell(\\beta_0, \\beta_1) = \\left( \\sum_{i=1}^{n} \\left( y_i - \\exp(\\beta_0 + \\beta_1 x_i) \\right), \\sum_{i=1}^{n} \\left( (y_i - \\exp(\\beta_0 + \\beta_1 x_i)) x_i \\right) \\right)^\\top\\]\n\n\nð¾ Problem 3.5\n\nCode two functions, one that evaluates \\(\\ell(\\beta_0, \\beta_1)\\), and another that evaluates its gradient, i.e.Â \\(\\nabla\\ell(\\beta_0, \\beta_1)\\). Note that the first function is scalar valued, whereas the second is vector valued (\\(2\\times 1\\) in our case).\n\nThe next code creates two functions, one that evaluates \\(\\ell(\\beta_0, \\beta_1)\\), and another that evaluates its gradient \\(\\nabla\\ell(\\beta_0, \\beta_1)\\).\n\n# Log-likelihood function\nLikelihood_fnct &lt;- function(b_0, b_1, X, y) \n  {\n  log_lhood &lt;- sum(y * (b_0 + b_1 * X) - exp(b_0 + b_1 * X) - log(factorial(y)))\n  return(log_lhood)\n}\n################################################################################################\n# Gradient of the log-likelihood function\nGradient_fnct &lt;- function(b_0, b_1, X, y) \n  {\n  grad_b_0 &lt;- sum(y - exp(b_0 + b_1 * X))\n  grad_b_1 &lt;- sum((y - exp(b_0 + b_1 * X)) * X)\n  \n  return(c(grad_b_0, grad_b_1))\n} \n\n\n\nð¾ Problem 3.6\n\nImplement gradient descent (Algorithm 5.1 in Lindholm et al.Â (2022)) to learn the parameters in the Poisson regression model by minimising the cost function \\(J(\\beta_0, \\beta_1)\\). Implement three different algorithms, with step sizes \\(\\gamma=0.01, 0.1, 0.25\\), and using \\((0,0)\\) as starting value. Note that Algorithm 5.1 has a stopping condition which states that it stops when the change in the parameter updates from one iteration to the other is small enough. If a wrong implementation is given, or the step size is chosen such that the algorithm does not converge, the code might get stuck in an infinite while loop (the condition to exit the loop is never met). For this reason, we here perform 200 iterations of the algorithm only (regardless of convergence or not). Comment on the results.\n\nThe next code implements gradient descent (Algorithm 5.1 in Lindholm et al.Â (2022)) to learn the parameters in the Poisson regression model by minimising the cost function \\(J(\\beta_0, \\beta_1)\\). It runs three different algorithms, with step sizes \\(\\gamma=0.01, 0.1, 0.25\\), and using \\((0,0)\\) as starting value. It has a stopping condition which states that it stops when the change in the parameter updates from one iteration to the other is small enough (0.000001). If a wrong implementation is given, or the step size is chosen such that the algorithm does not converge, the code might get stuck in an infinite while loop. For this reason, we here perform 200 iterations of the algorithm only. Moreover, the code plots the cost function per iteration, the Euclidean norm of the gradient of the cost function at each iteration and the values of each parameter against the number of iterations to inspect the effects of the three different learning rates.\n\n# Cost function \ncost_function &lt;- function(beta, x, y) {\n  n &lt;- length(y)\n  beta_0 &lt;- beta[1]\n  beta_1 &lt;- beta[2]\n  mu &lt;- exp(beta_0 + beta_1 * x)\n  -sum(y * (beta_0 + beta_1 * x) - mu) / n \n}\n\n# Gradient of the cost function\ngradient_function &lt;- function(beta, x, y) {\n  n &lt;- length(y)\n  beta_0 &lt;- beta[1]\n  beta_1 &lt;- beta[2]\n  mu &lt;- exp(beta_0 + beta_1 * x)\n  grad_beta_0 &lt;- -sum(y - mu) / n\n  grad_beta_1 &lt;- -sum((y - mu) * x) / n\n  c(grad_beta_0, grad_beta_1)\n}\n\n# Gradient Descent\ngradient_descent_algo &lt;- function(x, y, gamma, tol = 1e-6, max_iter = 200) {\n  beta &lt;- c(0, 0)  # Starting value \n  beta_prev &lt;- beta  # To store previous value of beta for stopping condition\n  cost_history &lt;- numeric(max_iter)  # To store the cost at each iteration\n  grad_norm_history &lt;- numeric(max_iter)  # To store the norm of the gradient at each iteration\n  beta_0_history &lt;- numeric(max_iter)  # To store beta_0 values\n  beta_1_history &lt;- numeric(max_iter)  # To store beta_1 values\n  \n  for (iter in 1:max_iter) {\n    cost &lt;- cost_function(beta, x, y)  # Current cost\n    grad &lt;- gradient_function(beta, x, y)  # Current gradient\n    beta &lt;- beta - gamma * grad  # Update the parameter\n    # Store history\n    cost_history[iter] &lt;- cost\n    grad_norm_history[iter] &lt;- norm(grad, type = \"2\")  # Euclidean norm of the gradient\n    beta_0_history[iter] &lt;- beta[1]\n    beta_1_history[iter] &lt;- beta[2]\n    # Stopping condition\n    if (norm(beta - beta_prev, type = \"2\") &lt; tol) {\n      break\n    }\n    beta_prev &lt;- beta  # Update previous beta\n  }\n  # Store info\n  list(beta = beta, cost_history = cost_history, grad_norm_history = grad_norm_history,\n       beta_0_history = beta_0_history, beta_1_history = beta_1_history, iter = iter)\n}\n\ngen_x &lt;- function(seed=123){\n  set.seed(seed)\n  return(runif(0, 1, n = 1000))\n}\n# Test\nx &lt;- gen_x()\n\n# Parameters\nb_0 &lt;- 0.3\nb_1 &lt;- 2.5\n\n# Calculate the lambda using the true model parameters\nlambda &lt;- exp(b_0 + b_1 * x)\n\n# Simulate y \ny &lt;- rpois(length(x), lambda) \n\n# Gradient descent algorithm for each gamma\ngammas &lt;- c(0.01, 0.1, 0.25)\n\n# Results for each gamma\nresults &lt;- lapply(gammas, function(gamma) gradient_descent_algo(x, y, gamma))\n\n# Plotting results for analysis\npar(mfrow = c(2, 3))\n\n# Cost function over iterations, it illustrates the convergence as it should settle when reaching the minimum. The faster it settles, the faster the convergence.\nfor (i in seq_along(gammas)) {\n  plot(results[[i]]$cost_history[1:results[[i]]$iter], type = \"l\", \n       main = paste(\"Cost function (gamma =\", gammas[i], \")\"),\n       xlab = \"Iteration\", ylab = \"Cost\")\n}\n\n# Gradient norm over iterations, these should eventually go to zero.\nfor (i in seq_along(gammas)) {\n  plot(results[[i]]$grad_norm_history[1:results[[i]]$iter], type = \"l\", \n       main = paste(\"Gradient norm (gamma =\", gammas[i], \")\"),\n       xlab = \"Iteration\", ylab = \"Gradient norm\")\n}\n\n\n\n\n\n\n\n# Parameter updates over iterations, these should converge to (the vicinity of) the true values.\nfor (i in seq_along(gammas)) {\n  plot(results[[i]]$beta_0_history[1:results[[i]]$iter], type = \"l\", col = \"cornflowerblue\", \n       ylim = range(c(results[[i]]$beta_0_history, results[[i]]$beta_1_history)),\n       xlab = \"Iteration\", ylab = \"Parameter Value\", main = paste(\"Beta Updates (gamma =\", gammas[i], \")\"))\n  lines(results[[i]]$beta_1_history[1:results[[i]]$iter], col = \"lightcoral\")\n  legend(\"topleft\", legend = c(\"Beta_0\", \"Beta_1\"), col = c(\"cornflowerblue\", \"lightcoral\"), lty = 1, cex = 0.5)\n}\n\n\n\n\n\n\n\n\nThe cost function over iterations for gamma = 0.01 decreases smoothly and steadily toward zero, but the convergence is slow, for gamma = 0.1 the cost decreases more quickly compared to gamma = 0.01 and converges in fewer iterations. Finally, for gamma = 0.25, the cost shows an erratic behavior, oscillating after a rapid initial decrease. As the learning rate is too large, the algorithm reaches the minimum and oscillate around it, without converging.\nFor gamma = 0.01, the gradient norm decreases smoothly over time, approaching zero around iteration 200, showing slow convergence. With gamma = 0.1, the gradient norm drops more rapidly and converges much faster, indicating that the gradient is approaching zero quickly. For gamma = 0.25 The gradient norm oscillates, mirroring the behavior of the cost function. This suggests that the algorithm is not converging due to the large step size.\nThe parameter values with gamma = 0.1 converge slowly over time. Both parameters gradually increase toward their true values (beta_0 = 0.3 and beta_1 = 2.5), but they require many iterations to stabilize. For gamma = 0.1, the parameters converge much faster compared to gamma = 0.01. Both beta_0 and beta_1 approach their true values within the first 50 iterations. With gamma = 0.25, the parameters exhibit severe oscillations. They start to converge toward the true values, but due to the large learning rate, the updates cause them to overshoot, resulting in oscillation without stabilizing. This confirms that gamma = 0.25 is too large.\nAs we can see, the gradient descent algorithmâs performance is highly dependent on the choice of the learning rate (gamma). A small learning rate (gamma = 0.01) results in slow but stable convergence, while a moderate rate (gamma = 0.1) offers the best balance, leading to fast and stable convergence. However, a large learning rate (gamma = 0.25) causes oscillations and prevents convergence due to overshooting the minimum. Therefore, gamma = 0.1 is the optimal choice for efficient and accurate parameter estimation in this scenario.\n\n\nð¾ Problem 3.7\n\nRun stochastic gradient descent (Algorithm 5.3 in Lindholm et al.Â (2022)) for \\(E=20\\) epochs to learn the parameters in the Poisson regression model. Experiment with three different mini-batch sizes, \\(n_b=10,50,100\\) and use the diminishing learning rate \\(\\gamma^{(t)}=0.5/t^{0.6}\\). Which mini-batch size seems to converge the fastest?\n\nThe next code Run stochastic gradient descent (Algorithm 5.3 in Lindholm et al.Â (2022)) for \\(E=20\\) epochs to learn the parameters in the Poisson regression model using with three different mini-batch sizes, \\(n_b=10,50,100\\) and with a diminishing learning rate \\(\\gamma^{(t)}=0.5/t^{0.6}\\)\n\n# Gradient function for a mini-batch\nmini_batch_gradient &lt;- function(beta, x_batch, y_batch) {\n  n &lt;- length(y_batch)\n  beta_0 &lt;- beta[1]\n  beta_1 &lt;- beta[2]\n  mu &lt;- exp(beta_0 + beta_1 * x_batch)\n  grad_beta_0 &lt;- -sum(y_batch - mu) / n\n  grad_beta_1 &lt;- -sum((y_batch - mu) * x_batch) / n\n  c(grad_beta_0, grad_beta_1)\n}\n\n# Cost function\ncost_function &lt;- function(beta, x, y) {\n  n &lt;- length(y)\n  beta_0 &lt;- beta[1]\n  beta_1 &lt;- beta[2]\n  mu &lt;- exp(beta_0 + beta_1 * x)  \n  log_likelihood &lt;- sum(y * log(mu) - mu) \n  return(-log_likelihood / n) \n}\n\n# Stochastic Gradient Descent\nsgd_algo &lt;- function(x, y, n_b, E = 20, tol = 1e-6) {\n  beta &lt;- c(0, 0)  \n  n &lt;- length(y)\n  t &lt;- 1  # Iteration counter\n  cost_history &lt;- list()\n  beta_history &lt;- list()\n  \n  for (epoch in 1:E) {\n    # Shuffle the data at the start of each epoch\n    indices &lt;- sample(1:n)\n    x_shuffled &lt;- x[indices]\n    y_shuffled &lt;- y[indices]\n    \n    for (i in seq(1, n, by = n_b)) {\n      # Mini-batch data\n      x_batch &lt;- x_shuffled[i:min(i+n_b-1, n)]\n      y_batch &lt;- y_shuffled[i:min(i+n_b-1, n)]\n      \n      # Learning rate\n      gamma_t &lt;- 0.5 / t^0.6\n      \n      # Compute the gradient for the mini-batch\n      grad &lt;- mini_batch_gradient(beta, x_batch, y_batch)\n      \n      # Update the parameters\n      beta &lt;- beta - gamma_t * grad\n      \n      # Compute the cost for this mini-batch\n      cost &lt;- cost_function(beta, x_batch, y_batch)\n      \n      # Store history\n      cost_history[[t]] &lt;- cost\n      beta_history[[t]] &lt;- beta\n      t &lt;- t + 1\n    }\n  }\n  \n  return(list(beta = beta, cost_history = cost_history, beta_history = beta_history))\n}\n\ngen_x &lt;- function(seed=123){\n  set.seed(seed)\n  return(runif(0, 1, n = 1000))\n}\n# Test\nx &lt;- gen_x()\n\n# Parameters\nb_0 &lt;- 0.3\nb_1 &lt;- 2.5\n\n# Calculate the Poisson rate (lambda) using the true model parameters\nlambda &lt;- exp(b_0 + b_1 * x)\n\n# Simulate y from a Poisson distribution with rate lambda\ny &lt;- rpois(length(x), lambda) \n\n# Mini-batch sizes\nmini_batch_sizes &lt;- c(10, 50, 100)\n\n# Run SGD \nresults_sgd &lt;- lapply(mini_batch_sizes, function(n_b) sgd_algo(x, y, n_b))\n\n# Plot\npar(mfrow = c(2, 3))\n\n# Cost function over iterations\nfor (i in seq_along(mini_batch_sizes)) {\n  plot(unlist(results_sgd[[i]]$cost_history), type = \"l\", \n       main = paste(\"Cost (mini-batch =\", mini_batch_sizes[i], \")\"),\n       xlab = \"Iteration\", ylab = \"Cost\")\n}\n\n# Parameter updates over iterations\nfor (i in seq_along(mini_batch_sizes)) {\n  betas &lt;- do.call(rbind, results_sgd[[i]]$beta_history)\n  plot(betas[, 1], type = \"l\", col = \"cornflowerblue\", \n       ylim = range(betas), main = paste(\"Beta Updates (mini-batch =\", mini_batch_sizes[i], \")\"),\n       xlab = \"Iteration\", ylab = \"Parameter Value\")\n  lines(betas[, 2], col = \"lightcoral\")\n  legend(\"bottomright\", legend = c(\"Beta_0\", \"Beta_1\"), col = c(\"cornflowerblue\", \"lightcoral\"), lty = 1, cex = 0.5)\n}\n\n\n\n\n\n\n\n\nAs the batch size increases the gradient estimates become more stable because they average over more data points, reducing noise. This leads to smoother updates and faster, more stable convergence. However, larger mini-batches trade off speed, as updates happen less frequently compared to smaller batches. As seen in the chart, using \\(n_b = 50\\) provide the fastest convergence while maintaining reasonable stability. It strikes a good balance between the noise when using \\(n_b = 10\\) and stability when using \\(n_b = 100\\)."
  },
  {
    "objectID": "index.html#learning-parametric-models-by-second-order-optimisation",
    "href": "index.html#learning-parametric-models-by-second-order-optimisation",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "4. Learning parametric models by second order optimisation",
    "text": "4. Learning parametric models by second order optimisation\n\nð¾ Problem 4.1\n\nDerive (analytically) the Hessian of the log-likelihood \\(\\ell(\\beta_0, \\beta_1)\\) for the Poisson regression model.\n\n\nLog-likelihood for Poisson regression\n\nThe Poisson likelihood for an observation \\(y_i\\) given \\(x_i\\) and the model parameters \\(\\beta_0\\), \\(\\beta_1\\) is:\n\\[\np(y_i | x_i, \\beta_0, \\beta_1) = \\frac{e^{-(\\mu_i)}(\\mu_i^{y_i})}{y_i!}\n\\]\nwhere \\(\\mu_i = e^{\\beta_0 + \\beta_1 x_i}\\)\nTaking the log of the likelihood for a single observation:\n\\[\n\\log p(y_i | x_i, \\beta_0, \\beta_1) = y_i (\\beta_0 + \\beta_1 x_i) - e^{\\beta_0 + \\beta_1 x_i} - \\log(y_i!)\n\\]\n\nFirst and Second Order Derivatives:\n\nNow, differentiate log-likelihood function with respect to \\(\\beta_0\\) and \\(\\beta_1\\) to get the first and second-order partial derivatives.\n* First derivative with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial}{\\partial \\beta_0} \\log p(y_i | x_i, \\beta_0, \\beta_1) = y_i - e^{\\beta_0 + \\beta_1 x_i}\n\\]\n* First derivative with respect to \\(\\beta_1\\):\n\\[\n\\frac{\\partial}{\\partial \\beta_1} \\log p(y_i | x_i, \\beta_0, \\beta_1) = x_i \\left( y_i - e^{\\beta_0 + \\beta_1 x_i} \\right)\n\\]\nSecond-order derivatives to obtain the Hessian matrix.\n* Second derivative with respect to \\(\\beta_0\\):\n\\[\n\\frac{\\partial^2}{\\partial \\beta_0^2} \\log p(y_i | x_i, \\beta_0, \\beta_1) = -e^{\\beta_0 + \\beta_1 x_i}\n\\]\n* Mixed second derivative:\n\\[\n\\frac{\\partial^2}{\\partial \\beta_0 \\partial \\beta_1} \\log p(y_i | x_i, \\beta_0, \\beta_1) = \\frac{\\partial}{\\partial \\beta_1} \\left( y_i - e^{\\beta_0 + \\beta_1 x_i} \\right) = -x_i e^{\\beta_0 + \\beta_1 x_i}\n\\]\n* Since the Hessian matrix is symmetric, we know that:\n\\[\n\\frac{\\partial^2}{\\partial \\beta_0 \\partial \\beta_1} = \\frac{\\partial^2}{\\partial \\beta_1 \\partial \\beta_0}\n\\]\n* Second derivative with respect to \\(\\beta_1\\):\n\\[\n\\frac{\\partial^2}{\\partial \\beta_1^2} \\log p(y_i | x_i, \\beta_0, \\beta_1) = -x_i^2 e^{\\beta_0 + \\beta_1 x_i}\n\\]\nThus, the Hessian matrix for a single observation is:\n\\[\n\\nabla\\nabla^\\top \\log p(y_i|x_i,\\beta_0, \\beta_1) =\\begin{bmatrix}\n-e^{\\beta_0 + \\beta_1 x_i} & -x_i e^{\\beta_0 + \\beta_1 x_i} \\\\\n-x_i e^{\\beta_0 + \\beta_1 x_i} & -x_i^2 e^{\\beta_0 + \\beta_1 x_i}\n\\end{bmatrix}\n\\]\n\n\nð¾ Problem 4.2\n\nFill in the NA values in the function Hess_log_dens_single_obs().\n\nThe next code has the NA values of the function Hess_log_dens_single_obs() filled with the calculated values for phi11, phi12 and phi22:\n\nHess_log_dens_single_obs &lt;- function(beta, y_i, X_i) {\n  beta_0 &lt;- beta[1]\n  beta_1 &lt;- beta[2]\n  mu &lt;- exp(beta_0 + beta_1 * X_i)  # Predicted mean\n  \n  # Hessian matrix from derived equation in 4.1\n  phi11 &lt;- -mu  \n  phi12 &lt;- -X_i * mu  \n  phi21 &lt;- phi12  \n  phi22 &lt;- -X_i^2 * mu  \n  \n  # Return the Hessian matrix for a single observation\n  return(matrix(c(phi11, phi21, phi12, phi22), nrow = 2, ncol = 2))\n}\n\nHess_log_like &lt;- function(beta, y, X) {\n  n &lt;- length(y)\n  sum_Hess_log_like &lt;- matrix(rep(0, 4), nrow = 2, ncol = 2)\n  for(i in 1:n) {\n    sum_Hess_log_like &lt;- sum_Hess_log_like + Hess_log_dens_single_obs(beta, y[i], X[i])\n  }\n  return(sum_Hess_log_like)\n}\n\n\n\nð¾ Problem 4.3\n\nImplement a trust region Newtonâs method (Algorithm 5.2 in Lindholm et al.Â (2022)) to learn the parameters in the Poisson regression model. Run the algorithm for 200 iterations using starting values \\((0,0)\\) and trust region radius \\(D=1\\). Compare the convergence of this method to that of the gradient descent method in Problem 3.6.\n\nThe next code implements a trust region Newtonâs method to learn the parameters in the Poisson regression model. It runs the algorithm for 200 iterations using starting values \\((0,0)\\) and trust region radius \\(D=1\\).\n\n# Gradient function \nGrad_log_like &lt;- function(beta, y, X) {\n  n &lt;- length(y)\n  grad &lt;- rep(0, length(beta))\n  for (i in 1:n) {\n    mu_i &lt;- exp(beta[1] + beta[2] * X[i])\n    grad[1] &lt;- grad[1] + (y[i] - mu_i)\n    grad[2] &lt;- grad[2] + X[i] * (y[i] - mu_i)\n  }\n  return(grad)\n}\n\n# Trust Region Newton's Method\nTrustRegionNewton &lt;- function(y, X, beta_init, max_iter = 200, D = 1) {\n  beta &lt;- beta_init\n  t &lt;- 0\n  tol &lt;- 1e-6  # tolerance for convergence\n  \n  while (t &lt; max_iter) {\n    # Gradient\n    grad &lt;- Grad_log_like(beta, y, X)\n    \n    # Hessian\n    hess &lt;- Hess_log_like(beta, y, X)\n    \n    # Newton step: v = - (Hessian)^(-1) * gradient\n    v &lt;- -solve(hess) %*% grad\n    \n    # Compute step size\n    v_norm &lt;- sqrt(sum(v^2))\n    eta &lt;- min(1, D / v_norm)\n    \n    # Update the parameters\n    beta_new &lt;- beta + eta * v\n    \n    # Check convergence\n    if (sqrt(sum((beta_new - beta)^2)) &lt; tol) {\n      break\n    }\n    \n    # Update beta\n    beta &lt;- beta_new\n    t &lt;- t + 1\n  }\n  \n  return(beta)\n}\n\ngen_x &lt;- function(seed=123){\n  set.seed(seed)\n  return(runif(0, 1, n = 1000))\n}\n# Test\nx &lt;- gen_x()\n\n# Parameters\nb_0 &lt;- 0.3\nb_1 &lt;- 2.5\n\n# Calculate lambda using the true model parameters\nlambda &lt;- exp(b_0 + b_1 * x)\n\n# Simulate y from a Poisson distribution with rate lambda\ny &lt;- rpois(length(x), lambda) \n\nbeta_init &lt;- c(0, 0) # Initial beta values (0, 0)\nfinal_beta &lt;- TrustRegionNewton(y = y, X = x, beta_init = beta_init)\n# Output\nfinal_beta\n\n          [,1]\n[1,] 0.2875776\n[2,] 2.5299341\n\n\nBy using the Hessian, the Trust Region Newtonâs Method adjusts the step size based on the local curvature of the cost function. This allows it to take more aggressive steps when the function is relatively flat and more cautious steps when the function is steep, leading to more efficient convergence."
  },
  {
    "objectID": "index.html#learning-parametric-models-using-the-optim-function",
    "href": "index.html#learning-parametric-models-using-the-optim-function",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "5. Learning parametric models using the optim function",
    "text": "5. Learning parametric models using the optim function\n\nð¾ Problem 5.1\n\nCode a cost function \\(J(\\beta)\\) that uses the vectorised functions above ('log_dens_Poisson()' and 'log_like_Poisson()').\n\n\nlog_dens_Poisson &lt;- function(beta, y, X) {\n  # log-density for each y_i, X_i\n  return(dpois(y, lambda = exp(X%*%beta),log = TRUE))\n}\n\nlog_like_Poisson &lt;- function(beta, y, X) {\n  # log-likelihood (for all observations)\n  return(sum(log_dens_Poisson(beta, y, X)))\n}\n\nSo the cost function \\(J(\\beta)\\) is the negative of Poisson log like function:\n\n##  ... to pass x,y because we don't have data frame\ncost_function &lt;- function(beta, ...) {\n\n  ##  For Passing the X and y\n  args &lt;- list(...)\n  y &lt;- args$y  \n  X &lt;- args$X \n  \n  # Negative log-likelihoo for minimizes \n  return(-log_like_Poisson(beta, y, X))\n}\n\n\n\nð¾ Problem 5.2\n\nUse the 'optim()' function to minimise the objective function \\(J(\\beta)\\). Can you retrieve the true parameter values?\n\nThe next code optimizes the beta for simulated data:\n\ngen_x &lt;- function(seed=123){\n  set.seed(seed)\n  return(runif(0, 1, n = 1000))\n}\n# Test\nx &lt;- gen_x()\n\n# Parameters\nb_0 &lt;- 0.3\nb_1 &lt;- 2.5\n\n# Calculate the Poisson rate (lambda) using the true model parameters\nlambda &lt;- exp(b_0 + b_1 * x)\n\n# Simulate y from a Poisson distribution with rate lambda\ny &lt;- rpois(length(x), lambda) \n\n## Adding Intercept with one coloum of constant 1\nX &lt;- cbind(1,x)  # x in to 1 *1000 shape matrix\n\n# Starting values for beta \nbeta_start &lt;- c(0, 0) \n\n# optim() function to minimize J(beta)\nresult &lt;- optim(par = beta_start, fn = cost_function, method = \"L-BFGS-B\", y = y, X = X)\n\n# Optimized parameters\nbeta_optim &lt;- result$par\ncat ('Original Beta values : ', 0.3,2.5 , '\\n')\n\nOriginal Beta values :  0.3 2.5 \n\ncat('Optimal Beta values : ',beta_optim)\n\nOptimal Beta values :  0.2875846 2.529925\n\n\nAs shown in the code below we can retrieve the parameter values using 'optim()'.\n\n\nð¾ Problem 5.3\n\nUse the 'optim()' function to learn the parameters in the Poisson regression model for the eBay dataset. Note that you have to read in the dataset yourself this time.\n\nThe next code optimize beta for eBay data:\n\n# Load the dataset\nload(file = '/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 2/eBay_coins.RData')\n\n# Extract y : nBids and feature matrix x: everything but nbids\ny &lt;- eBay_coins$nBids\nX &lt;- as.matrix(eBay_coins[,!names(eBay_coins) %in% c(\"nBids\")])\n\n# Starting values for beta\nbeta_start &lt;- rep(0, ncol(X)) # 9 , 0s\n\n# Use the optim() function to minimize cost function\nresult &lt;- optim(beta_start, cost_function, method = \"L-BFGS-B\", y = y, X = X)\n\n# Optimized parameters\nbeta_optim &lt;- result$par\ncat('Optimal Beta : ',beta_optim)\n\nOptimal Beta :  1.072417 -0.02057349 -0.3945853 0.4439136 -0.05219661 -0.2207668 0.07068201 -0.1207006 -1.89411\n\n\n\n\nð¾ Problem 5.4\n\nSuppose there is a new auction with features:\n\n'PowerSeller'=1,\n'VerifyID'=1,\n'Sealed'=1,\n'MinBlem'=0,\n'MajBlem'=0,\n'LargNeg'=0,\n'LogBook'=1,\n'MinBidShare'=0.5.\n\nProvide a point estimate of the expected number of bidders for this auction.\n\nIn Poisson regression, the expected value (mean) is given by:\n\\[\\hat{\\mu} = \\exp(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n)\\] Where \\(\\hat{\\mu}\\) is the expected number of bidders, and \\(\\beta\\) is the vector of estimated parameters, and x is the feature vector.\n\n# New auction features\nnew_features &lt;- c(1, 1, 1, 1, 0, 0, 0, 1, 0.5)  # Including intercept as the first element\n\n# Compute the expected number of bidders\nexpected_bidders &lt;- exp(sum(beta_optim * new_features))\n\n#Provide a point estimate of the expected number of bidders for this auction.\ncat('Number of Bidders : ',round(expected_bidders))\n\nNumber of Bidders :  1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this ML session we treat topics such as as bagging, boosting, and learning parametric models by several optimisation algorithms. Here we consider modelling the number of rides each hour for a bike rental company; we also create and test a spam e-mails filter, and try to estimate the number of bidders at an eBay auction of coins."
  }
]