---
title: "Machine Learning: Mathematical Theory and Applications"
subtitle: ""
date: last-modified
format: 
  html:
    self-contained: true
toc: true
execute:
  error: false
theme: Default
title-block-banner-color: Primary
editor: visual
---

```{=html}
<style>
.boxed-text {
  border: 2px solid black;
  padding: 10px;
  margin: 10px 0;
}
</style>
```
```{r setup, include=FALSE}
# Set global chunk options
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## 1. Bagging and boosting for bike rental data (regression)

#### ðŸ‘¾ Problem 1.1

::: boxed-text
Use the `randomForest()` function in the `randomForest` package to fit three random forest regressions (bagging algorithms) with the `ntree` argument set to, respectively, 50, 100, and 500 (number of trees to grow). In each case, compute the root mean squared error (RMSE) for the training and test datasets.
:::

The following code prepares the dataset and sets the training and testing data:

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
suppressMessages(library(dplyr))
suppressMessages(library(splines))
bike_data <- read.csv('/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 2/bike_rental_hourly.csv')
bike_data$log_cnt <- log(bike_data$cnt)
bike_data$hour <- bike_data$hr/23 # transform [0, 23] to [0, 1]. 0 is midnight, 1 is 11 PM

# One hot for weathersit
one_hot_encode_weathersit <- model.matrix(~ as.factor(weathersit) - 1,data = bike_data)
one_hot_encode_weathersit  <- one_hot_encode_weathersit[, -1] # Remove reference category
colnames(one_hot_encode_weathersit) <- c('cloudy', 'light rain', 'heavy rain')
bike_data <- cbind(bike_data, one_hot_encode_weathersit)

# One hot for weekday
one_hot_encode_weekday <- model.matrix(~ as.factor(weekday) - 1,data = bike_data)
one_hot_encode_weekday  <- one_hot_encode_weekday[, -1] # Remove reference category
colnames(one_hot_encode_weekday) <- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')
bike_data <- cbind(bike_data, one_hot_encode_weekday)

# One hot for weekday
one_hot_encode_season <- model.matrix(~ as.factor(season) - 1,data = bike_data)
one_hot_encode_season  <- one_hot_encode_season[, -1] # Remove reference category
colnames(one_hot_encode_season) <- c('Spring', 'Summer', 'Fall')
bike_data <- cbind(bike_data, one_hot_encode_season)

# Create lags
bike_data_new <- mutate(bike_data, lag1 = lag(log_cnt, 1), lag2 = lag(log_cnt, 2), 
                        lag3 = lag(log_cnt, 3), lag4 = lag(log_cnt, 4), lag24 = lag(log_cnt, 24))

bike_data_new <- bike_data_new[-c(1:24),] # Lost 24 obs because of lagging

# Create training and test data
bike_all_data_train <- bike_data_new[bike_data_new$dteday >= as.Date("2011-01-01") & bike_data_new$dteday <=  as.Date("2012-05-31"), ]
bike_all_data_test <- bike_data_new[bike_data_new$dteday >= as.Date("2012-06-01") & bike_data_new$dteday <=  as.Date("2012-12-31"), ]
X_train <- cbind(1, bike_all_data_train[, c("lag1", "lag2",  "lag3", "lag4", "lag24")])
spline_basis <- ns(bike_all_data_train$hour, df = 10, intercept = FALSE)
X_train <- cbind(X_train, spline_basis)
colnames(X_train)[1] <- "intercept"
knots <- attr(spline_basis, "knots")
variables_to_keep_in_X <- c("yr", "holiday", "workingday", "temp", "atemp", "hum", "windspeed") 
variables_to_keep_in_X <- c(variables_to_keep_in_X, colnames(one_hot_encode_weathersit), colnames(one_hot_encode_weekday), colnames(one_hot_encode_season))
X_train <- cbind(X_train, bike_all_data_train[, variables_to_keep_in_X])
# Training data
X_train <- as.matrix(X_train)
y_train <- bike_all_data_train$log_cnt
# Test data
y_test <- bike_all_data_test$log_cnt
X_test <- cbind(1, bike_all_data_test[, c("lag1", "lag2",  "lag3", "lag4", "lag24")])
spline_basis_test <- ns(bike_all_data_test$hour, df=10, knots=knots, intercept = FALSE)
X_test <- cbind(X_test, spline_basis_test)
colnames(X_test)[1] <- "intercept"
X_test <- cbind(X_test, bike_all_data_test[, variables_to_keep_in_X])
X_test <- as.matrix(X_test)
```

The following code uses the `randomForest()` function in the `randomForest` package to fit three random forest regressions (bagging algorithms) with the `ntree` argument set to, respectively, 50, 100, and 500 (number of trees to grow). Then it computes the RMSE for the training and test datasets:

```{r}
# Load library
suppressMessages(library(randomForest))

# Random Forest with ntree = 50
set.seed(123)
Forest_cincuenta <- randomForest(X_train, y_train, ntree = 50)
y_train_cincuenta <- predict(Forest_cincuenta, X_train)
y_test_cincuenta <- predict(Forest_cincuenta, X_test)

# RMSE for ntree = 50
rmse_train_cincuenta <- sqrt(mean((y_train - y_train_cincuenta)^2))
rmse_test_cincuenta <- sqrt(mean((y_test - y_test_cincuenta)^2))

# Random Forest with ntree = 100
Forest_cien <- randomForest(X_train, y_train, ntree = 100)
y_train_cien <- predict(Forest_cien, X_train)
y_test_cien <- predict(Forest_cien, X_test)

# RMSE for ntree = 100
rmse_train_cien <- sqrt(mean((y_train - y_train_cien)^2))
rmse_test_cien <- sqrt(mean((y_test - y_test_cien)^2))

# Random Forest with ntree = 500
Forest_quinientos <- randomForest(X_train, y_train, ntree = 500)
y_train_quinientos <- predict(Forest_quinientos, X_train)
y_test_quinientos <- predict(Forest_quinientos, X_test)

# RMSE for ntree = 500
rmse_train_quinientos <- sqrt(mean((y_train - y_train_quinientos)^2))
rmse_test_quinientos <- sqrt(mean((y_test - y_test_quinientos)^2))

# Print RMSE values
print(paste("RMSE for ntree = 50 on the training data:", rmse_train_cincuenta))
print(paste("RMSE for ntree = 50 on the test data:", rmse_test_cincuenta))
print(paste("RMSE for ntree = 100 on the training data:", rmse_train_cien))
print(paste("RMSE for ntree = 100 on the test data:", rmse_test_cien))
print(paste("RMSE for ntree = 500 on the training data:", rmse_train_quinientos))
print(paste("RMSE for ntree = 500 on the test data:", rmse_test_quinientos))
```

#### ðŸ‘¾ Problem 1.2

::: boxed-text
Plot a time series plot of the response in the original scale (i.e. counts and not log-counts) for the last week of the test data (last $24\times 7$ observations). In the same figure, plot a time series plot of the fitted values (in the original scale) from Problem 1.1 when using `ntree=50` and `ntree=100.` Comment on the results.
:::

The next code plots a time series plot of the response in the original scale for the last week of the test data, in the same figure, plots a time series plot of the fitted values (in the original scale) from Problem 1.1 when using `ntree=50` and `ntree=100.`:

```{r}
# Llast week test data
last_week_test <- tail(bike_all_data_test, 24 * 7)

# Predicted values for ntree = 50 and ntree = 100 in original scale
y_test_pred_cincuenta_org <- exp(y_test_cincuenta)
y_test_pred_cien_org <- exp(y_test_cien)

# Last week of predicted values
last_week_pred_cincuenta <- tail(y_test_pred_cincuenta_org, 24 * 7)
last_week_pred_cien <- tail(y_test_pred_cien_org, 24 * 7)

# Last week bike counts in the original scale
last_week_y_test <- tail(exp(y_test), 24 * 7)

# Plot
plot(last_week_y_test, type = "l", col = "green", lwd = 2, xlab = "Time in hours", ylab = "Number of bike rentals", main = "Time Series Plot of Actual and Predicted Bike Rentals for the Last Week", cex.main = 0.8)
lines(last_week_pred_cincuenta, col = "cornflowerblue", lwd = 2, lty = 2)
lines(last_week_pred_cien, col = "lightcoral", lwd = 2, lty = 3)

# Legend
legend("topleft", legend = c("Actual counts", "Predicted (ntree = 50)", "Predicted (ntree = 100)"), col = c("green", "cornflowerblue", "lightcoral"), lty = c(1, 2, 3), lwd = 2, cex = 0.8)
```

As we can see in the chart, when we use a larger number of trees to grow, the predicted number of bike rides approximates better the actual number of bike rides in the peaks. This happens because more trees allow for better generalization and lower variance in the predictions, which results in the model capturing the peaks more accurately.

#### ðŸ‘¾ Problem 1.3

::: boxed-text
Use the `xgboost()` function in the `xgboost` package to fit three extreme gradient boosting regressions (boosting algorithms) with the `nrounds` argument set to, respectively, 10, 25, and 50 (max number of boosting iterations). In each case, compute the root mean squared error (RMSE) for the training and test datasets.
:::

The next code uses the `xgboost()` function in the `xgboost` package to fit three extreme gradient boosting regressions with the `nrounds` argument set to, respectively, 10, 25, and 50. Moreover, the code computes and prints the root mean squared error for the training and test datasets:

```{r}
suppressMessages(library(xgboost))

# Convert the data to a compact format for xgboost, we use y_train as label
xgtrain <- xgb.DMatrix(data = X_train, label = y_train)

# Using xgboost with 10 iterations using a regression with squared loss
xgb_diez <- xgboost(data = xgtrain, nrounds = 10, objective = "reg:squarederror", verbose = 0)

# Predict
y_train_xgb_diez <- predict(xgb_diez, newdata = X_train)
y_test_xgb_diez <- predict(xgb_diez, newdata = X_test)

# Using xgboost with 25 iterations using a regression with squared loss
xgb_venticinco <- xgboost(data = xgtrain, nrounds = 25, objective = "reg:squarederror", verbose = 0)

# Predict
y_train_xgb_venticinco <- predict(xgb_venticinco, newdata = X_train)
y_test_xgb_venticinco <- predict(xgb_venticinco, newdata = X_test)

# Using xgboost with 50 iterations using a regression with squared loss
xgb_cincuenta <- xgboost(data = xgtrain, nrounds = 50, objective = "reg:squarederror", verbose = 0)

# Predictions for training and test data
y_train_xgb_cincuenta <- predict(xgb_cincuenta, newdata = X_train)
y_test_xgb_cincuenta <- predict(xgb_cincuenta, newdata = X_test)

# Calculating and printing RMSE values
rmse_train_diez_xgb <- sqrt(mean((y_train - y_train_xgb_diez)^2))
rmse_test_diez_xgb <- sqrt(mean((y_test - y_test_xgb_diez)^2))
rmse_train_venticinco_xgb <- sqrt(mean((y_train - y_train_xgb_venticinco)^2))
rmse_test_venticinco_xgb <- sqrt(mean((y_test - y_test_xgb_venticinco)^2))
rmse_train_cincuenta_xgb <- sqrt(mean((y_train - y_train_xgb_cincuenta)^2))
rmse_test_cincuenta_xgb <- sqrt(mean((y_test - y_test_xgb_cincuenta)^2))
print(paste("RMSE for nrounds = 10 on the training data:", rmse_train_diez_xgb))
print(paste("RMSE for nrounds = 10 on the test data:", rmse_test_diez_xgb))
print(paste("RMSE for nrounds = 25 on the training data:", rmse_train_venticinco_xgb))
print(paste("RMSE for nrounds = 25 on the test data:", rmse_test_venticinco_xgb))
print(paste("RMSE for nrounds = 50 on the training data:", rmse_train_cincuenta_xgb))
print(paste("RMSE for nrounds = 50 on the test data:", rmse_test_cincuenta_xgb))
```

#### ðŸ‘¾ Problem 1.4

::: boxed-text
Plot the response in the original scale for the last week of the test data (last $24\times 7$ observations) vs the corresponding fitted values from the best bagging model in Problem 1.2 and the best boosting model in Problem 1.3. Comment on the results.
:::

The next code plots the response in the original scale for the last week of the test data vs the corresponding fitted values from the best bagging model in Problem 1.2 (ntree =500, lowest RMSE on test data) and the best boosting model in Problem 1.3 (nrounds =50, lowest RMSE on test data):

```{r}
# Predicted values in original scale
rf_top_original <- exp(y_test_quinientos)
xgb_top_original <- exp(y_test_xgb_cincuenta)

# Last week of predicted values for each model
last_week_rf_top_original <- tail(rf_top_original, 24 * 7)
last_week_xgb_top_original <- tail(xgb_top_original, 24 * 7)

# Plot
plot(last_week_y_test, type = "l", col = "green", lwd = 2, xlab = "Time in hours", ylab = "Number of Bike Rentals", 
     main = "Actual vs Predicted Bike Rentals for the Last Week", cex.main = 0.8, ylim = range(c(last_week_y_test, last_week_rf_top_original, last_week_xgb_top_original)))
lines(last_week_rf_top_original, col = "cornflowerblue", lwd = 2, lty = 2)
lines(last_week_xgb_top_original, col = "lightcoral", lwd = 2, lty = 3)

# Legend
legend("topleft", legend = c("Actual counts", "Random Forest (ntree = 500)", "XGBoost (nrounds = 50)"), 
       col = c("green", "cornflowerblue", "lightcoral"), lty = c(1, 2, 3), lwd = 2, cex = 0.7)
```

We can appreciate how XGBoost seems to handle better the dynamic changes in the number of rentals, particularly during rapid increases and decreases, but overestimating the peaks. In the other hand, random forest performs more conservatively but struggles with capturing sharp increases (the averaging of trees reduces variance but introduce more bias).

## 2. Bagging and boosting for spam email data (classification)

#### ðŸ‘¾ Problem 2.1

::: boxed-text
Use the `randomForest()` function in the `randomForest` package to fit three random forest classification with the `ntree` argument set to, respectively, 50, 100, and 500. In each case, plot the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package `pROC()` (all plots in the same figure). Comment on the results.
:::

The following code prepares the dataset and sets the training and testing data:

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
load(file = '/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 2/spam_ham_emails.RData')
Spam_ham_emails[, -1] <- scale(Spam_ham_emails[, -1])
Spam_ham_emails['spam'] <- as.factor(Spam_ham_emails['spam'] == 1) # Changing from 1->TRUE, 0->FALSE
levels(Spam_ham_emails$spam) <- c("not spam", "spam")
suppressMessages(library(caret))
train_obs <- createDataPartition(y = Spam_ham_emails$spam, p = .75, list = FALSE)
train <- Spam_ham_emails[train_obs, ]
y_train <- train$spam
X_train <- train[, -1]
test <- Spam_ham_emails[-train_obs, ]
y_test <- test$spam
X_test <- test[, -1]

# Confirm both training and test are balanced with respect to spam emails
cat("Percentage of training data consisting of spam emails:", 
              100*mean(train$spam == "spam"))
cat("Percentage of test data consisting of spam emails:", 
              100*mean(test$spam == "spam"))
```

The next code uses the `randomForest()` function in the `randomForest` package to fit three random forest classification with the `ntree` argument set to, respectively, 50, 100, and 500. Moreover, the code plots the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package `pROC()`:

```{r}
suppressMessages(library(pROC))
suppressMessages(library(randomForest))
# Train 3 diffrent random forest models wites varing ntree values
rf_50 <- randomForest(spam ~ ., data=train, importance=TRUE,proximity=TRUE,ntree=50)
rf_100 <- randomForest(spam ~ ., data=train, importance=TRUE,proximity=TRUE,ntree=100)
rf_500 <- randomForest(spam ~ ., data=train, importance=TRUE,proximity=TRUE,ntree=500)

# Generate predictions for different random forest models with varying ntree values
prob_rf_50 <- predict(rf_50, test, type = "prob")[, "spam"]
prob_rf_100 <- predict(rf_100, test, type = "prob")[, "spam"]
prob_rf_500 <- predict(rf_500, test, type = "prob")[, "spam"]

# Compute ROC curves for each model
roc_rf_50 <- roc(y_test, prob_rf_50, levels = c("not spam", "spam"), direction = "<")
roc_rf_100 <- roc(y_test, prob_rf_100, levels = c("not spam", "spam"), direction = "<")
roc_rf_500 <- roc(y_test, prob_rf_500, levels = c("not spam", "spam"), direction = "<")

# Plot the ROC curves in the same plot
plot(roc_rf_50, col = "cornflowerblue", main = "ROC Curves for Random Forest Models with Different ntree Values")
lines(roc_rf_100, col = "mediumseagreen")
lines(roc_rf_500, col = "lightcoral")
legend("bottomright", legend = c("ntree = 50", "ntree = 100", "ntree = 500"), col = c("cornflowerblue", "mediumseagreen", "lightcoral"), lwd = 1)

# Compute AUC for each model
auc_rf_50 <- auc(roc_rf_50)
auc_rf_100 <- auc(roc_rf_100)
auc_rf_500 <- auc(roc_rf_500)

# Print AUC values for each model
cat("AUC for ntree = 50:", auc_rf_50, "\n")
cat("AUC for ntree = 100:", auc_rf_100, "\n")
cat("AUC for ntree = 500:", auc_rf_500, "\n")
```

The AUC difference between `ntree = 50` and `ntree = 500` is less than 0.001, so using fewer trees is more computationally efficient, as it provides almost the same classification performance than using many trees, but with less computational overhead.

As the ROC curves show, increasing the number of trees from 50 to 500 does not significantly change the curve, so adding more trees doesn't bring added benefit in terms of ROC performance.

#### ðŸ‘¾ Problem 2.2

::: boxed-text
Predict the test data using the random forest classifier in Problem 2.1 with `ntree=100` following the rule: if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, and compute the confusion matrix using the `confusionMatrix()` function from the `caret` package.
:::

The next code predicts the test data using the random forest classifier in Problem 2.1 with `ntree=100` following the rule: if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, it also computes the confusion matrix using the `confusionMatrix()` function from the `caret` package.

```{r}
threshold <- 0.5 # Predict spam if probability > threshold
y_rf_100 <- as.factor(prob_rf_100 > threshold) 
levels(y_rf_100) <- c("not spam", "spam")

##  confusionMatrix for ntree=100
confusionMatrix(data = y_rf_100, y_test, positive = "spam")
```

#### ðŸ‘¾ Problem 2.3

::: boxed-text
Use the `xgboost()` function in the `xgboost` package to fit three extreme gradient boosting regressions with the `nrounds` argument set to, respectively, 10, 25, and 50. In each case, plot the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package `pROC()` (all plots in the same figure). Moreover, add the ROC plot for the random forest in Problem 2.1 with `ntree=100`. Comment on the results.
:::

The next code uses the `xgboost()` function in the `xgboost` package to fit three extreme gradient boosting regressions with the `nrounds` argument set to, respectively, 10, 25, and 50. In each case,it plots the ROC curve with the ROC curve of the random forest of Problem 2.1 and computes the area under the ROC curves (AUC) for the test data using the package `pROC()`.

```{r}
X_train_xgb <- as.matrix(X_train)
X_test_xgb <- as.matrix(X_test)
y_train_xgb <- as.integer(y_train) - 1
y_test_xgb <- as.integer(y_test) - 1

library(xgboost)
xg_10 <- xgboost(data = X_train_xgb, label = y_train_xgb, max.depth = 2, eta = 1, nthread = 2, nrounds = 10, objective = "binary:logistic")

xg_25 <- xgboost(data = X_train_xgb, label = y_train_xgb, max.depth = 2, eta = 1, nthread = 2, nrounds = 25, objective = "binary:logistic")

xg_50 <- xgboost(data = X_train_xgb, label = y_train_xgb, max.depth = 2, eta = 1, nthread = 2, nrounds = 50, objective = "binary:logistic")

# Caluculate the pobability
xg_prob_10 <- predict(xg_10, X_test_xgb)
xg_prob_25 <- predict(xg_25, X_test_xgb)
xg_prob_50 <- predict(xg_50, X_test_xgb)

# Compute ROC curves for each model
roc_xg_10 <- roc(y_test, xg_prob_10, levels = c("not spam", "spam"), direction = "<")
roc_xg_25 <- roc(y_test, xg_prob_25, levels = c("not spam", "spam"), direction = "<")
roc_xg_50 <- roc(y_test, xg_prob_50, levels = c("not spam", "spam"), direction = "<")

# Plot the ROC curves 
plot(roc_xg_10, col = "cornflowerblue", main = "ROC Curves for XGboost Models with Different ntree Values")
lines(roc_xg_25, col = "mediumseagreen")
lines(roc_xg_50, col = "lightcoral")
lines(roc_rf_100, col = "yellow")
legend("bottomright", legend = c("nrounds = 10", "nrounds = 25", "nrounds = 50","ntree = 100"), col = c("cornflowerblue", "mediumseagreen", "lightcoral",'yellow'), lwd = 1)

# Compute AUC for each model
auc_xg_10 <- auc(roc_xg_10)
auc_xg_25 <- auc(roc_xg_25)
auc_xg_50 <- auc(roc_xg_50)

# Print AUC values for each model
cat("AUC for nrounds = 10:", auc_xg_10, "\n")
cat("AUC for nrounds = 25:", auc_xg_25, "\n")
cat("AUC for nrounds = 50:", auc_xg_50, "\n")
```

Similar to the behavior observed in Random Forest models, increasing nrounds in XGBoost shows diminishing returns. While boosting iterations can improve the model's learning capacity, after a certain point, the AUC reaches a plateau, and further increases in nrounds provide minimum improvements, so it would be more computationally efficient to use fewer rounds.

#### ðŸ‘¾ Problem 2.4

::: boxed-text
Predict the test data using the extreme gradient boosting classifier in Problem 2.3 with `nrounds=25` following the rule: if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, and compute the confusion matrix using the `confusionMatrix()` function from the `caret` package.
:::

The next code predicts the test data using the extreme gradient boosting classifier in Problem 2.3 with `nrounds=25` following the rule: if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, and computes the confusion matrix using the `confusionMatrix()` function from the `caret` package.

```{r}
threshold <- 0.5 # Predict spam if probability > 50%
y_xg_25 <- as.factor(xg_prob_25 > threshold) 
levels(y_xg_25) <- c("not spam", "spam")

##  confusionMatrix for ntree=100
confusionMatrix(data = y_xg_25, y_test, positive = "spam")
```

## 3. Learning parametric models by gradient based optimisation

#### ðŸ‘¾ Problem 3.1

::: boxed-text
Simulate $n=1000$ independent (conditionally on $x$) $y$ observations from the Poisson regression model $$y|\beta_0,\beta_1 \sim \mathrm{Poisson}(\exp\left(\beta_0 + \beta_1x \right)),$$ with the true parameters $\beta_0= 0.3$ and $\beta_1= 2.5$.
:::

The next code simulates $n=1000$ independent (conditionally on $x$), $y$ observations from the Poisson regression model $$y|\beta_0,\beta_1 \sim \mathrm{Poisson}(\exp\left(\beta_0 + \beta_1x \right)),$$ with the true parameters $\beta_0= 0.3$ and $\beta_1= 2.5$.

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
gen_x <- function(seed=123){
  set.seed(seed)
  return(runif(0, 1, n = 1000))
}
# Test
x <- gen_x()

# Parameters
b_0 <- 0.3
b_1 <- 2.5

# Calculate the Poisson rate (lambda) using the true model parameters
lambda <- exp(b_0 + b_1 * x)

# Simulate y from a Poisson distribution with rate lambda
y <- rpois(length(x), lambda) 
```

#### ðŸ‘¾ Problem 3.2

::: boxed-text
In the same figure, plot the scatter $\{x_i, y_i\}_{i=1}^n$ and the conditional expected value $\mathbb{E}(y|x)$ as a function of $x$ given the true parameter values.
:::

The next code plots the scatter $\{x_i, y_i\}_{i=1}^n$ and the conditional expected value $\mathbb{E}(y|x)$ as a function of $x$ given the true parameter values.

```{r}
# Scatter plot of the observed x and y values
plot(x, y, main = "Scatter plot of x and y with Conditional Expected Value",
     xlab = "x", ylab = "y", pch = 20, col = "cornflowerblue", cex = 0.5)

# Create x_grid for evaluating the conditional expected value
x_grid <- seq(0, 1, length.out = 10000)

# Calculate the conditional expected value E(y|x) = exp(b_0 + b_1 * x)
expected_y <- exp(b_0 + b_1 * x_grid)

# Add the expected value curve to the plot
lines(x_grid, expected_y, col = "lightcoral", lwd = 2)

# Add a legend
legend("topleft", legend = c("Observed y", "E(y|x)"), 
       col = c("cornflowerblue", "lightcoral"), pch = c(20, NA), lty = c(NA, 1), lwd = c(NA, 2))
```

#### ðŸ‘¾ Problem 3.3

::: boxed-text
Derive (analytically) the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.
:::

1.  **Probability mass function**

The probability mass function of a Poisson distribution is given by:

$$
p(y_i|\mu_i) = \frac{\mu_i^{y_i} \exp(-\mu_i)}{y_i!}
$$

where $\mu_i$ is the mean for each $y_i$.

In a Poisson regression model, the expected value $\mu_i$ is given by:

$$
\mu_i = \exp(\beta_0 + \beta_1 x_i)
$$

Then, we can write the probability mass function of $y_i$, conditional on $x_i$ as:

$$
p(y_i | x_i, \beta_0, \beta_1) = \frac{\left(\exp(\beta_0 + \beta_1 x_i)\right)^{y_i} \exp\left(-\exp(\beta_0 + \beta_1 x_i)\right)}{y_i!}
$$

2.  **Log-likelihood function**

The log-likelihood function for the entire dataset is the log of the joint probability of observing all $y_i$'s, assuming independence of the $y_i$'s, conditional on the $x_i$'s.

Thus, the log-likelihood $\ell(\beta_0, \beta_1)$ is:

$$
\ell(\beta_0, \beta_1) = \sum_{i=1}^{n} \log \left( p(y_i | x_i, \beta_0, \beta_1) \right)
$$

Substituting the probability mass function of the Poisson distribution into $\ell(\beta_0, \beta_1)$, we get:

$$
\ell(\beta_0, \beta_1) = \sum_{i=1}^{n} \log \left( \frac{\left(\exp(\beta_0 + \beta_1 x_i)\right)^{y_i} \exp\left(-\exp(\beta_0 + \beta_1 x_i)\right)}{y_i!} \right)
$$

Using the properties of the logarithms we can simplify $\ell(\beta_0, \beta_1)$ as:

$$
\ell(\beta_0, \beta_1) = \sum_{i=1}^{n} \left( y_i (\beta_0 + \beta_1 x_i) - \exp(\beta_0 + \beta_1 x_i) - \log(y_i!) \right)
$$

This log-likelihood function can be maximized with respect to $\beta_0$ and $\beta_1$ to estimate the parameters using numerical optimization methods, such as gradient descent or other gradient-based optimizers.

#### ðŸ‘¾ Problem 3.4

::: boxed-text
Derive (analytically) the gradient of the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.
:::

We know that because of the (conditional) independence assumption, $\nabla \ell(\beta_0, \beta_1) = \sum_{i=1}^{n} \nabla \log p(y_i | x_i, \beta_0, \beta_1)$ and noting that $\nabla \log p(y_i|x_i,\beta_0, \beta_1)=\left(\frac{\partial}{\partial\beta_0}\log p(y_i|x_i,\beta_0, \beta_1), \frac{\partial}{\partial\beta_1}\log p(y_i|x_i,\beta_0, \beta_1) \right)^\top.$ Then we can find $\nabla \ell(\beta_0, \beta_1)$:

The log of the Poisson probability mass function is: $\log p(y_i | x_i, \beta_0, \beta_1) = y_i (\beta_0 + \beta_1 x_i) - \exp(\beta_0 + \beta_1 x_i) - \log(y_i!)$, then we can calculate the gradient respect to to each $\beta$:

The gradient with respect to $\beta_0$:

$$\frac{\partial}{\partial \beta_0} \log p(y_i | x_i, \beta_0, \beta_1) = y_i - \exp(\beta_0 + \beta_1 x_i)$$

The total gradient with respect to $\beta_0$:

$$\frac{\partial \ell(\beta_0, \beta_1)}{\partial \beta_0} = \sum_{i=1}^{n} \left( y_i - \exp(\beta_0 + \beta_1 x_i) \right)$$

The gradient with respect to $\beta_1$:

$$\frac{\partial}{\partial \beta_1} \log p(y_i | x_i, \beta_0, \beta_1) = x_i \left( y_i - \exp(\beta_0 + \beta_1 x_i) \right) $$

The total gradient with respect to $\beta_1$:

$$\frac{\partial \ell(\beta_0, \beta_1)}{\partial \beta_1} = \sum_{i=1}^{n} \left(x_i(y_i - \exp(\beta_0 + \beta_1 x_i))  \right)$$

Now we can re-write $\nabla \ell(\beta_0, \beta_1)$ as:

$$\nabla \ell(\beta_0, \beta_1) = \left( \sum_{i=1}^{n} \left( y_i - \exp(\beta_0 + \beta_1 x_i) \right), \sum_{i=1}^{n} \left( (y_i - \exp(\beta_0 + \beta_1 x_i)) x_i \right) \right)^\top$$

#### ðŸ‘¾ Problem 3.5

::: boxed-text
Code two functions, one that evaluates $\ell(\beta_0, \beta_1)$, and another that evaluates its gradient, i.e. $\nabla\ell(\beta_0, \beta_1)$. Note that the first function is scalar valued, whereas the second is vector valued ($2\times 1$ in our case).
:::

The next code creates two functions, one that evaluates $\ell(\beta_0, \beta_1)$, and another that evaluates its gradient $\nabla\ell(\beta_0, \beta_1)$.

```{r}
# Log-likelihood function
Likelihood_fnct <- function(b_0, b_1, X, y) 
  {
  log_lhood <- sum(y * (b_0 + b_1 * X) - exp(b_0 + b_1 * X) - log(factorial(y)))
  return(log_lhood)
}
################################################################################################
# Gradient of the log-likelihood function
Gradient_fnct <- function(b_0, b_1, X, y) 
  {
  grad_b_0 <- sum(y - exp(b_0 + b_1 * X))
  grad_b_1 <- sum((y - exp(b_0 + b_1 * X)) * X)
  
  return(c(grad_b_0, grad_b_1))
} 
```

#### ðŸ‘¾ Problem 3.6

::: boxed-text
Implement gradient descent (Algorithm 5.1 in Lindholm et al. (2022)) to learn the parameters in the Poisson regression model by minimising the cost function $J(\beta_0, \beta_1)$. Implement three different algorithms, with step sizes $\gamma=0.01, 0.1, 0.25$, and using $(0,0)$ as starting value. Note that Algorithm 5.1 has a stopping condition which states that it stops when the change in the parameter updates from one iteration to the other is small enough. If a wrong implementation is given, or the step size is chosen such that the algorithm does not converge, the code might get stuck in an infinite while loop (the condition to exit the loop is never met). For this reason, we here perform 200 iterations of the algorithm only (regardless of convergence or not). Comment on the results.
:::

The next code implements gradient descent (Algorithm 5.1 in Lindholm et al. (2022)) to learn the parameters in the Poisson regression model by minimising the cost function $J(\beta_0, \beta_1)$. It runs three different algorithms, with step sizes $\gamma=0.01, 0.1, 0.25$, and using $(0,0)$ as starting value. It has a stopping condition which states that it stops when the change in the parameter updates from one iteration to the other is small enough (0.000001). If a wrong implementation is given, or the step size is chosen such that the algorithm does not converge, the code might get stuck in an infinite while loop. For this reason, we here perform 200 iterations of the algorithm only. Moreover, the code plots the cost function per iteration, the Euclidean norm of the gradient of the cost function at each iteration and the values of each parameter against the number of iterations to inspect the effects of the three different learning rates.

```{r}
# Cost function 
cost_function <- function(beta, x, y) {
  n <- length(y)
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  mu <- exp(beta_0 + beta_1 * x)
  -sum(y * (beta_0 + beta_1 * x) - mu) / n 
}

# Gradient of the cost function
gradient_function <- function(beta, x, y) {
  n <- length(y)
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  mu <- exp(beta_0 + beta_1 * x)
  grad_beta_0 <- -sum(y - mu) / n
  grad_beta_1 <- -sum((y - mu) * x) / n
  c(grad_beta_0, grad_beta_1)
}

# Gradient Descent
gradient_descent_algo <- function(x, y, gamma, tol = 1e-6, max_iter = 200) {
  beta <- c(0, 0)  # Starting value 
  beta_prev <- beta  # To store previous value of beta for stopping condition
  cost_history <- numeric(max_iter)  # To store the cost at each iteration
  grad_norm_history <- numeric(max_iter)  # To store the norm of the gradient at each iteration
  beta_0_history <- numeric(max_iter)  # To store beta_0 values
  beta_1_history <- numeric(max_iter)  # To store beta_1 values
  
  for (iter in 1:max_iter) {
    cost <- cost_function(beta, x, y)  # Current cost
    grad <- gradient_function(beta, x, y)  # Current gradient
    beta <- beta - gamma * grad  # Update the parameter
    # Store history
    cost_history[iter] <- cost
    grad_norm_history[iter] <- norm(grad, type = "2")  # Euclidean norm of the gradient
    beta_0_history[iter] <- beta[1]
    beta_1_history[iter] <- beta[2]
    # Stopping condition
    if (norm(beta - beta_prev, type = "2") < tol) {
      break
    }
    beta_prev <- beta  # Update previous beta
  }
  # Store info
  list(beta = beta, cost_history = cost_history, grad_norm_history = grad_norm_history,
       beta_0_history = beta_0_history, beta_1_history = beta_1_history, iter = iter)
}

gen_x <- function(seed=123){
  set.seed(seed)
  return(runif(0, 1, n = 1000))
}
# Test
x <- gen_x()

# Parameters
b_0 <- 0.3
b_1 <- 2.5

# Calculate the lambda using the true model parameters
lambda <- exp(b_0 + b_1 * x)

# Simulate y 
y <- rpois(length(x), lambda) 

# Gradient descent algorithm for each gamma
gammas <- c(0.01, 0.1, 0.25)

# Results for each gamma
results <- lapply(gammas, function(gamma) gradient_descent_algo(x, y, gamma))

# Plotting results for analysis
par(mfrow = c(2, 3))

# Cost function over iterations, it illustrates the convergence as it should settle when reaching the minimum. The faster it settles, the faster the convergence.
for (i in seq_along(gammas)) {
  plot(results[[i]]$cost_history[1:results[[i]]$iter], type = "l", 
       main = paste("Cost function (gamma =", gammas[i], ")"),
       xlab = "Iteration", ylab = "Cost")
}

# Gradient norm over iterations, these should eventually go to zero.
for (i in seq_along(gammas)) {
  plot(results[[i]]$grad_norm_history[1:results[[i]]$iter], type = "l", 
       main = paste("Gradient norm (gamma =", gammas[i], ")"),
       xlab = "Iteration", ylab = "Gradient norm")
}

# Parameter updates over iterations, these should converge to (the vicinity of) the true values.
for (i in seq_along(gammas)) {
  plot(results[[i]]$beta_0_history[1:results[[i]]$iter], type = "l", col = "cornflowerblue", 
       ylim = range(c(results[[i]]$beta_0_history, results[[i]]$beta_1_history)),
       xlab = "Iteration", ylab = "Parameter Value", main = paste("Beta Updates (gamma =", gammas[i], ")"))
  lines(results[[i]]$beta_1_history[1:results[[i]]$iter], col = "lightcoral")
  legend("topleft", legend = c("Beta_0", "Beta_1"), col = c("cornflowerblue", "lightcoral"), lty = 1, cex = 0.5)
}
```

The cost function over iterations for gamma = 0.01 decreases smoothly and steadily toward zero, but the convergence is slow, for gamma = 0.1 the cost decreases more quickly compared to gamma = 0.01 and converges in fewer iterations. Finally, for gamma = 0.25, the cost shows an erratic behavior, oscillating after a rapid initial decrease. As the learning rate is too large, the algorithm reaches the minimum and oscillate around it, without converging.

For gamma = 0.01, the gradient norm decreases smoothly over time, approaching zero around iteration 200, showing slow convergence. With gamma = 0.1, the gradient norm drops more rapidly and converges much faster, indicating that the gradient is approaching zero quickly. For gamma = 0.25 The gradient norm oscillates, mirroring the behavior of the cost function. This suggests that the algorithm is not converging due to the large step size.

The parameter values with gamma = 0.1 converge slowly over time. Both parameters gradually increase toward their true values (beta_0 = 0.3 and beta_1 = 2.5), but they require many iterations to stabilize. For gamma = 0.1, the parameters converge much faster compared to gamma = 0.01. Both beta_0 and beta_1 approach their true values within the first 50 iterations. With gamma = 0.25, the parameters exhibit severe oscillations. They start to converge toward the true values, but due to the large learning rate, the updates cause them to overshoot, resulting in oscillation without stabilizing. This confirms that gamma = 0.25 is too large.

As we can see, the gradient descent algorithm's performance is highly dependent on the choice of the learning rate (gamma). A small learning rate (gamma = 0.01) results in slow but stable convergence, while a moderate rate (gamma = 0.1) offers the best balance, leading to fast and stable convergence. However, a large learning rate (gamma = 0.25) causes oscillations and prevents convergence due to overshooting the minimum. Therefore, gamma = 0.1 is the optimal choice for efficient and accurate parameter estimation in this scenario.

#### ðŸ‘¾ Problem 3.7

::: boxed-text
Run stochastic gradient descent (Algorithm 5.3 in Lindholm et al. (2022)) for $E=20$ epochs to learn the parameters in the Poisson regression model. Experiment with three different mini-batch sizes, $n_b=10,50,100$ and use the diminishing learning rate $\gamma^{(t)}=0.5/t^{0.6}$. Which mini-batch size seems to converge the fastest?
:::

The next code Run stochastic gradient descent (Algorithm 5.3 in Lindholm et al. (2022)) for $E=20$ epochs to learn the parameters in the Poisson regression model using with three different mini-batch sizes, $n_b=10,50,100$ and with a diminishing learning rate $\gamma^{(t)}=0.5/t^{0.6}$

```{r}

# Gradient function for a mini-batch
mini_batch_gradient <- function(beta, x_batch, y_batch) {
  n <- length(y_batch)
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  mu <- exp(beta_0 + beta_1 * x_batch)
  grad_beta_0 <- -sum(y_batch - mu) / n
  grad_beta_1 <- -sum((y_batch - mu) * x_batch) / n
  c(grad_beta_0, grad_beta_1)
}

# Cost function
cost_function <- function(beta, x, y) {
  n <- length(y)
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  mu <- exp(beta_0 + beta_1 * x)  
  log_likelihood <- sum(y * log(mu) - mu) 
  return(-log_likelihood / n) 
}

# Stochastic Gradient Descent
sgd_algo <- function(x, y, n_b, E = 20, tol = 1e-6) {
  beta <- c(0, 0)  
  n <- length(y)
  t <- 1  # Iteration counter
  cost_history <- list()
  beta_history <- list()
  
  for (epoch in 1:E) {
    # Shuffle the data at the start of each epoch
    indices <- sample(1:n)
    x_shuffled <- x[indices]
    y_shuffled <- y[indices]
    
    for (i in seq(1, n, by = n_b)) {
      # Mini-batch data
      x_batch <- x_shuffled[i:min(i+n_b-1, n)]
      y_batch <- y_shuffled[i:min(i+n_b-1, n)]
      
      # Learning rate
      gamma_t <- 0.5 / t^0.6
      
      # Compute the gradient for the mini-batch
      grad <- mini_batch_gradient(beta, x_batch, y_batch)
      
      # Update the parameters
      beta <- beta - gamma_t * grad
      
      # Compute the cost for this mini-batch
      cost <- cost_function(beta, x_batch, y_batch)
      
      # Store history
      cost_history[[t]] <- cost
      beta_history[[t]] <- beta
      t <- t + 1
    }
  }
  
  return(list(beta = beta, cost_history = cost_history, beta_history = beta_history))
}

gen_x <- function(seed=123){
  set.seed(seed)
  return(runif(0, 1, n = 1000))
}
# Test
x <- gen_x()

# Parameters
b_0 <- 0.3
b_1 <- 2.5

# Calculate the Poisson rate (lambda) using the true model parameters
lambda <- exp(b_0 + b_1 * x)

# Simulate y from a Poisson distribution with rate lambda
y <- rpois(length(x), lambda) 

# Mini-batch sizes
mini_batch_sizes <- c(10, 50, 100)

# Run SGD 
results_sgd <- lapply(mini_batch_sizes, function(n_b) sgd_algo(x, y, n_b))

# Plot
par(mfrow = c(2, 3))

# Cost function over iterations
for (i in seq_along(mini_batch_sizes)) {
  plot(unlist(results_sgd[[i]]$cost_history), type = "l", 
       main = paste("Cost (mini-batch =", mini_batch_sizes[i], ")"),
       xlab = "Iteration", ylab = "Cost")
}

# Parameter updates over iterations
for (i in seq_along(mini_batch_sizes)) {
  betas <- do.call(rbind, results_sgd[[i]]$beta_history)
  plot(betas[, 1], type = "l", col = "cornflowerblue", 
       ylim = range(betas), main = paste("Beta Updates (mini-batch =", mini_batch_sizes[i], ")"),
       xlab = "Iteration", ylab = "Parameter Value")
  lines(betas[, 2], col = "lightcoral")
  legend("bottomright", legend = c("Beta_0", "Beta_1"), col = c("cornflowerblue", "lightcoral"), lty = 1, cex = 0.5)
}

```

As the batch size increases the gradient estimates become more stable because they average over more data points, reducing noise. This leads to smoother updates and faster, more stable convergence. However, larger mini-batches trade off speed, as updates happen less frequently compared to smaller batches. As seen in the chart, using $n_b = 50$ provide the fastest convergence while maintaining reasonable stability. It strikes a good balance between the noise when using $n_b = 10$ and stability when using $n_b = 100$.

## 4. Learning parametric models by second order optimisation

#### ðŸ‘¾ Problem 4.1

::: boxed-text
Derive (analytically) the Hessian of the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.
:::

1.  **Log-likelihood for Poisson regression**

The Poisson likelihood for an observation $y_i$ given $x_i$ and the model parameters $\beta_0$, $\beta_1$ is:

$$
 p(y_i | x_i, \beta_0, \beta_1) = \frac{e^{-(\mu_i)}(\mu_i^{y_i})}{y_i!}
$$

where $\mu_i = e^{\beta_0 + \beta_1 x_i}$

Taking the log of the likelihood for a single observation:

$$
\log p(y_i | x_i, \beta_0, \beta_1) = y_i (\beta_0 + \beta_1 x_i) - e^{\beta_0 + \beta_1 x_i} - \log(y_i!)
$$

2.  **First and Second Order Derivatives:**

Now, differentiate log-likelihood function with respect to $\beta_0$ and $\beta_1$ to get the first and second-order partial derivatives.

\* First derivative with respect to $\beta_0$:

$$
\frac{\partial}{\partial \beta_0} \log p(y_i | x_i, \beta_0, \beta_1) = y_i - e^{\beta_0 + \beta_1 x_i}
$$

\* First derivative with respect to $\beta_1$:

$$
\frac{\partial}{\partial \beta_1} \log p(y_i | x_i, \beta_0, \beta_1) = x_i \left( y_i - e^{\beta_0 + \beta_1 x_i} \right)
$$

Second-order derivatives to obtain the Hessian matrix.

\* Second derivative with respect to $\beta_0$:

$$
\frac{\partial^2}{\partial \beta_0^2} \log p(y_i | x_i, \beta_0, \beta_1) = -e^{\beta_0 + \beta_1 x_i}
$$

\* Mixed second derivative:

$$
\frac{\partial^2}{\partial \beta_0 \partial \beta_1} \log p(y_i | x_i, \beta_0, \beta_1) = \frac{\partial}{\partial \beta_1} \left( y_i - e^{\beta_0 + \beta_1 x_i} \right) = -x_i e^{\beta_0 + \beta_1 x_i}
$$

\* Since the Hessian matrix is symmetric, we know that:

$$
\frac{\partial^2}{\partial \beta_0 \partial \beta_1} = \frac{\partial^2}{\partial \beta_1 \partial \beta_0}
$$

\* Second derivative with respect to $\beta_1$:

$$
\frac{\partial^2}{\partial \beta_1^2} \log p(y_i | x_i, \beta_0, \beta_1) = -x_i^2 e^{\beta_0 + \beta_1 x_i}
$$

Thus, the Hessian matrix for a single observation is:

$$
\nabla\nabla^\top \log p(y_i|x_i,\beta_0, \beta_1) =\begin{bmatrix}
-e^{\beta_0 + \beta_1 x_i} & -x_i e^{\beta_0 + \beta_1 x_i} \\
-x_i e^{\beta_0 + \beta_1 x_i} & -x_i^2 e^{\beta_0 + \beta_1 x_i}
\end{bmatrix}
$$

#### ðŸ‘¾ Problem 4.2

::: boxed-text
Fill in the `NA` values in the function `Hess_log_dens_single_obs()`.
:::

The next code has the NA values of the function `Hess_log_dens_single_obs()` filled with the calculated values for phi11, phi12 and phi22:

```{r}
Hess_log_dens_single_obs <- function(beta, y_i, X_i) {
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  mu <- exp(beta_0 + beta_1 * X_i)  # Predicted mean
  
  # Hessian matrix from derived equation in 4.1
  phi11 <- -mu  
  phi12 <- -X_i * mu  
  phi21 <- phi12  
  phi22 <- -X_i^2 * mu  
  
  # Return the Hessian matrix for a single observation
  return(matrix(c(phi11, phi21, phi12, phi22), nrow = 2, ncol = 2))
}

Hess_log_like <- function(beta, y, X) {
  n <- length(y)
  sum_Hess_log_like <- matrix(rep(0, 4), nrow = 2, ncol = 2)
  for(i in 1:n) {
    sum_Hess_log_like <- sum_Hess_log_like + Hess_log_dens_single_obs(beta, y[i], X[i])
  }
  return(sum_Hess_log_like)
}
```

#### ðŸ‘¾ Problem 4.3

::: boxed-text
Implement a trust region Newton's method (Algorithm 5.2 in Lindholm et al. (2022)) to learn the parameters in the Poisson regression model. Run the algorithm for 200 iterations using starting values $(0,0)$ and trust region radius $D=1$. Compare the convergence of this method to that of the gradient descent method in Problem 3.6.
:::

The next code implements a trust region Newton's method to learn the parameters in the Poisson regression model. It runs the algorithm for 200 iterations using starting values $(0,0)$ and trust region radius $D=1$.

```{r}
# Gradient function 
Grad_log_like <- function(beta, y, X) {
  n <- length(y)
  grad <- rep(0, length(beta))
  for (i in 1:n) {
    mu_i <- exp(beta[1] + beta[2] * X[i])
    grad[1] <- grad[1] + (y[i] - mu_i)
    grad[2] <- grad[2] + X[i] * (y[i] - mu_i)
  }
  return(grad)
}

# Trust Region Newton's Method
TrustRegionNewton <- function(y, X, beta_init, max_iter = 200, D = 1) {
  beta <- beta_init
  t <- 0
  tol <- 1e-6  # tolerance for convergence
  
  while (t < max_iter) {
    # Gradient
    grad <- Grad_log_like(beta, y, X)
    
    # Hessian
    hess <- Hess_log_like(beta, y, X)
    
    # Newton step: v = - (Hessian)^(-1) * gradient
    v <- -solve(hess) %*% grad
    
    # Compute step size
    v_norm <- sqrt(sum(v^2))
    eta <- min(1, D / v_norm)
    
    # Update the parameters
    beta_new <- beta + eta * v
    
    # Check convergence
    if (sqrt(sum((beta_new - beta)^2)) < tol) {
      break
    }
    
    # Update beta
    beta <- beta_new
    t <- t + 1
  }
  
  return(beta)
}

gen_x <- function(seed=123){
  set.seed(seed)
  return(runif(0, 1, n = 1000))
}
# Test
x <- gen_x()

# Parameters
b_0 <- 0.3
b_1 <- 2.5

# Calculate lambda using the true model parameters
lambda <- exp(b_0 + b_1 * x)

# Simulate y from a Poisson distribution with rate lambda
y <- rpois(length(x), lambda) 

beta_init <- c(0, 0) # Initial beta values (0, 0)
final_beta <- TrustRegionNewton(y = y, X = x, beta_init = beta_init)
# Output
final_beta
```

By using the Hessian, the Trust Region Newton's Method adjusts the step size based on the local curvature of the cost function. This allows it to take more aggressive steps when the function is relatively flat and more cautious steps when the function is steep, leading to more efficient convergence.

## 5. Learning parametric models using the optim function

#### ðŸ‘¾ Problem 5.1

::: boxed-text
Code a cost function $J(\beta)$ that uses the vectorised functions above (`'log_dens_Poisson()'` and `'log_like_Poisson()'`).
:::

```{r}

log_dens_Poisson <- function(beta, y, X) {
  # log-density for each y_i, X_i
  return(dpois(y, lambda = exp(X%*%beta),log = TRUE))
}

log_like_Poisson <- function(beta, y, X) {
  # log-likelihood (for all observations)
  return(sum(log_dens_Poisson(beta, y, X)))
}
```

So the cost function $J(\beta)$ is the negative of Poisson log like function:

```{r}
##  ... to pass x,y because we don't have data frame
cost_function <- function(beta, ...) {

  ##  For Passing the X and y
  args <- list(...)
  y <- args$y  
  X <- args$X 
  
  # Negative log-likelihoo for minimizes 
  return(-log_like_Poisson(beta, y, X))
}
```

#### ðŸ‘¾ Problem 5.2

::: boxed-text
Use the `'optim()'` function to minimise the objective function $J(\beta)$. Can you retrieve the true parameter values?
:::

The next code optimizes the beta for simulated data:

```{r}

gen_x <- function(seed=123){
  set.seed(seed)
  return(runif(0, 1, n = 1000))
}
# Test
x <- gen_x()

# Parameters
b_0 <- 0.3
b_1 <- 2.5

# Calculate the Poisson rate (lambda) using the true model parameters
lambda <- exp(b_0 + b_1 * x)

# Simulate y from a Poisson distribution with rate lambda
y <- rpois(length(x), lambda) 

## Adding Intercept with one coloum of constant 1
X <- cbind(1,x)  # x in to 1 *1000 shape matrix

# Starting values for beta 
beta_start <- c(0, 0) 

# optim() function to minimize J(beta)
result <- optim(par = beta_start, fn = cost_function, method = "L-BFGS-B", y = y, X = X)

# Optimized parameters
beta_optim <- result$par
cat ('Original Beta values : ', 0.3,2.5 , '\n')
cat('Optimal Beta values : ',beta_optim)
```

As shown in the code below we can retrieve the parameter values using `'optim()'`.

#### ðŸ‘¾ Problem 5.3

::: boxed-text
Use the `'optim()'` function to learn the parameters in the Poisson regression model for the eBay dataset. Note that you have to read in the dataset yourself this time.
:::

The next code optimize beta for eBay data:

```{r}
# Load the dataset
load(file = '/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 2/eBay_coins.RData')

# Extract y : nBids and feature matrix x: everything but nbids
y <- eBay_coins$nBids
X <- as.matrix(eBay_coins[,!names(eBay_coins) %in% c("nBids")])

# Starting values for beta
beta_start <- rep(0, ncol(X)) # 9 , 0s

# Use the optim() function to minimize cost function
result <- optim(beta_start, cost_function, method = "L-BFGS-B", y = y, X = X)

# Optimized parameters
beta_optim <- result$par
cat('Optimal Beta : ',beta_optim)
```

#### ðŸ‘¾ Problem 5.4

::: boxed-text
Suppose there is a new auction with features:

-   `'PowerSeller'=1`,

-   `'VerifyID'=1`,

-   `'Sealed'=1`,

-   `'MinBlem'=0`,

-   `'MajBlem'=0`,

-   `'LargNeg'=0`,

-   `'LogBook'=1`,

-   `'MinBidShare'=0.5`.

Provide a point estimate of the expected number of bidders for this auction.
:::

In Poisson regression, the expected value (mean) is given by:

$$\hat{\mu} = \exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n)$$ Where $\hat{\mu}$ is the expected number of bidders, and $\beta$ is the vector of estimated parameters, and x is the feature vector.

```{r}
# New auction features
new_features <- c(1, 1, 1, 1, 0, 0, 0, 1, 0.5)  # Including intercept as the first element

# Compute the expected number of bidders
expected_bidders <- exp(sum(beta_optim * new_features))

#Provide a point estimate of the expected number of bidders for this auction.
cat('Number of Bidders : ',round(expected_bidders))
```
